{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QWERTY_LOW_LAYOUT: list[list[str]] = [\n",
    "    [\"`\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"-\", \"=\", \"<back>\"],\n",
    "    [\"<tab>\", \"q\", \"w\", \"e\", \"r\", \"t\", \"y\", \"u\", \"i\", \"o\", \"p\", \"[\", \"]\", \"\\\\\"],\n",
    "    [\n",
    "        \"<caps>\",\n",
    "        \"a\",\n",
    "        \"s\",\n",
    "        \"d\",\n",
    "        \"f\",\n",
    "        \"g\",\n",
    "        \"h\",\n",
    "        \"j\",\n",
    "        \"k\",\n",
    "        \"l\",\n",
    "        \";\",\n",
    "        \"'\",\n",
    "        \"<enter>\",\n",
    "        \"<enter>\",\n",
    "    ],\n",
    "    [\n",
    "        \"<shift>\",\n",
    "        \"<shift>\",\n",
    "        \"z\",\n",
    "        \"x\",\n",
    "        \"c\",\n",
    "        \"v\",\n",
    "        \"b\",\n",
    "        \"n\",\n",
    "        \"m\",\n",
    "        \",\",\n",
    "        \".\",\n",
    "        \"/\",\n",
    "        \"<shift>\",\n",
    "        \"<shift>\",\n",
    "    ],\n",
    "    [\n",
    "        \"<ctrl>\",\n",
    "        \"<alt>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<alt>\",\n",
    "        \"<ctrl>\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "QWERTY_HIGH_LAYOUT: list[list[str]] = [\n",
    "    [\"~\", \"!\", \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"(\", \")\", \"_\", \"+\", \"<back>\"],\n",
    "    [\"<tab>\", \"Q\", \"W\", \"E\", \"R\", \"T\", \"Y\", \"U\", \"I\", \"O\", \"P\", \"{\", \"}\", \"|\"],\n",
    "    [\n",
    "        \"<caps>\",\n",
    "        \"A\",\n",
    "        \"S\",\n",
    "        \"D\",\n",
    "        \"F\",\n",
    "        \"G\",\n",
    "        \"H\",\n",
    "        \"J\",\n",
    "        \"K\",\n",
    "        \"L\",\n",
    "        \":\",\n",
    "        '\"',\n",
    "        \"<enter>\",\n",
    "        \"<enter>\",\n",
    "    ],\n",
    "    [\n",
    "        \"<shift>\",\n",
    "        \"<shift>\",\n",
    "        \"Z\",\n",
    "        \"X\",\n",
    "        \"C\",\n",
    "        \"V\",\n",
    "        \"B\",\n",
    "        \"N\",\n",
    "        \"M\",\n",
    "        \"<\",\n",
    "        \">\",\n",
    "        \"?\",\n",
    "        \"<shift>\",\n",
    "        \"<shift>\",\n",
    "    ],\n",
    "    [\n",
    "        \"<ctrl>\",\n",
    "        \"<alt>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<space>\",\n",
    "        \"<alt>\",\n",
    "        \"<ctrl>\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 14, 14, 14, 14, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len(QWERTY_HIGH_LAYOUT),\n",
    "    len(QWERTY_HIGH_LAYOUT[0]),\n",
    "    len(QWERTY_HIGH_LAYOUT[1]),\n",
    "    len(QWERTY_HIGH_LAYOUT[2]),\n",
    "    len(QWERTY_HIGH_LAYOUT[3]),\n",
    "    len(QWERTY_HIGH_LAYOUT[4]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buttons_set(\n",
    "    low_layout: list[list[str]], high_layout: list[list[str]]\n",
    ") -> set[str]:\n",
    "    buttons: set[str] = set()\n",
    "\n",
    "    for layout in [low_layout, high_layout]:\n",
    "        for i in range(len(layout)):\n",
    "            for btn in layout[i]:\n",
    "                buttons.add(btn)\n",
    "\n",
    "    return buttons\n",
    "\n",
    "\n",
    "def get_keyboard_shape(layout: list[list[str]]) -> tuple[int, ...]:\n",
    "    shape = [len(row) for row in layout]\n",
    "\n",
    "    return tuple(shape)\n",
    "\n",
    "\n",
    "BUTTONS_SET = get_buttons_set(QWERTY_LOW_LAYOUT, QWERTY_HIGH_LAYOUT)\n",
    "KEYBOARD_LAYOUT_SHAPE = get_keyboard_shape(QWERTY_LOW_LAYOUT)\n",
    "KEYBOARD_LAYOUT_CUMSUM_SHAPE = np.cumsum(KEYBOARD_LAYOUT_SHAPE)\n",
    "KEYS = sum(KEYBOARD_LAYOUT_SHAPE) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 67\n",
    "PAIR_TO_ID = {}\n",
    "ID_TO_PAIR = {}\n",
    "c = 0\n",
    "for i in range(134):\n",
    "    for j in range(i + 1, 134):\n",
    "        PAIR_TO_ID[(i, j)] = c\n",
    "        ID_TO_PAIR[c] = (i, j)\n",
    "        c += 1\n",
    "\n",
    "ACTION_SPACE_SIZE = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8911"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTION_SPACE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_buttons(buttons: set[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    letters_dict = {}\n",
    "    for idx, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\"):\n",
    "        letters_dict[letter] = idx + 1\n",
    "        # letters_dict[letter.upper()] = -(idx + 1)\n",
    "\n",
    "    offset = len(letters_dict)\n",
    "    for idx, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\".upper()):\n",
    "        letters_dict[letter] = offset + idx + 1\n",
    "        # letters_dict[letter.upper()] = -(idx + 1)\n",
    "\n",
    "    encode_value = len(letters_dict) + 1  # (len(letters_dict) // 2) + 1\n",
    "    encode_dict = {}\n",
    "    decode_dict = {}\n",
    "    for btn in buttons:\n",
    "        if btn in letters_dict:\n",
    "            decode_dict[letters_dict[btn]] = btn\n",
    "            encode_dict[btn] = letters_dict[btn]\n",
    "        else:\n",
    "            decode_dict[encode_value] = btn\n",
    "            encode_dict[btn] = encode_value\n",
    "            encode_value += 1\n",
    "    return encode_dict, decode_dict\n",
    "\n",
    "\n",
    "ENCODE_DICT, DECODE_DICT = encode_decode_buttons(BUTTONS_SET)\n",
    "ENCODED_BUTTONS_SET = {ENCODE_DICT[btn] for btn in BUTTONS_SET}\n",
    "SHIFT_CODE = ENCODE_DICT[\"<shift>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'|': 53,\n",
       " '4': 54,\n",
       " '5': 55,\n",
       " 'p': 16,\n",
       " '>': 56,\n",
       " ')': 57,\n",
       " '$': 58,\n",
       " 'X': 50,\n",
       " '~': 59,\n",
       " '<space>': 60,\n",
       " 'O': 41,\n",
       " '?': 61,\n",
       " '`': 62,\n",
       " 'l': 12,\n",
       " '!': 63,\n",
       " 'b': 2,\n",
       " 'U': 47,\n",
       " '8': 64,\n",
       " '<tab>': 65,\n",
       " 'E': 31,\n",
       " '<alt>': 66,\n",
       " '{': 67,\n",
       " 'c': 3,\n",
       " '^': 68,\n",
       " 'v': 22,\n",
       " 'S': 45,\n",
       " '-': 69,\n",
       " ';': 70,\n",
       " 'a': 1,\n",
       " '}': 71,\n",
       " '\"': 72,\n",
       " 'K': 37,\n",
       " '<ctrl>': 73,\n",
       " '+': 74,\n",
       " 'P': 42,\n",
       " '#': 75,\n",
       " 'A': 27,\n",
       " 'm': 13,\n",
       " ':': 76,\n",
       " 'j': 10,\n",
       " '<shift>': 77,\n",
       " 'h': 8,\n",
       " 'u': 21,\n",
       " 'H': 34,\n",
       " 'd': 4,\n",
       " 'n': 14,\n",
       " 'R': 44,\n",
       " 'C': 29,\n",
       " '@': 78,\n",
       " 'Z': 52,\n",
       " 'x': 24,\n",
       " '0': 79,\n",
       " '\\\\': 80,\n",
       " 'G': 33,\n",
       " 'k': 11,\n",
       " '[': 81,\n",
       " '=': 82,\n",
       " 'V': 48,\n",
       " 'B': 28,\n",
       " '<back>': 83,\n",
       " 't': 20,\n",
       " '<caps>': 84,\n",
       " '(': 85,\n",
       " 'J': 36,\n",
       " 'e': 5,\n",
       " 'T': 46,\n",
       " '&': 86,\n",
       " '1': 87,\n",
       " '%': 88,\n",
       " 'Y': 51,\n",
       " '_': 89,\n",
       " ']': 90,\n",
       " '7': 91,\n",
       " 'y': 25,\n",
       " '<': 92,\n",
       " '*': 93,\n",
       " '9': 94,\n",
       " 'W': 49,\n",
       " 'o': 15,\n",
       " 'z': 26,\n",
       " ',': 95,\n",
       " 'q': 17,\n",
       " 'M': 39,\n",
       " '/': 96,\n",
       " 'I': 35,\n",
       " 'D': 30,\n",
       " 'g': 7,\n",
       " 'w': 23,\n",
       " '6': 97,\n",
       " 'Q': 43,\n",
       " 'N': 40,\n",
       " 'F': 32,\n",
       " '<enter>': 98,\n",
       " 'L': 38,\n",
       " '.': 99,\n",
       " 'r': 18,\n",
       " '2': 100,\n",
       " 'f': 6,\n",
       " 'i': 9,\n",
       " \"'\": 101,\n",
       " '3': 102,\n",
       " 's': 19}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENCODE_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for btn in BUTTONS_SET:\n",
    "    assert btn == DECODE_DICT[ENCODE_DICT[btn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BUTTONS_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layout = list[list[int]]\n",
    "\n",
    "\n",
    "def encode_layout(layout: list[list[str]]) -> Layout:\n",
    "    return [[ENCODE_DICT[btn] for btn in layout[i]] for i in range(len(layout))]\n",
    "\n",
    "\n",
    "def decode_layout(layout: Layout) -> list[list[str]]:\n",
    "    return [[DECODE_DICT[btn] for btn in layout[i]] for i in range(len(layout))]\n",
    "\n",
    "\n",
    "QWERTY_ENCODED_HIGH: Layout = encode_layout(QWERTY_HIGH_LAYOUT)\n",
    "QWERTY_ENCODED_LOW: Layout = encode_layout(QWERTY_LOW_LAYOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_buttons_encoded(high_layout: Layout, low_layout: Layout) -> list[int]:\n",
    "    all_buttons = []\n",
    "\n",
    "    for layout in (low_layout, high_layout):\n",
    "        for row in layout:\n",
    "            all_buttons.extend(row)\n",
    "    return all_buttons\n",
    "\n",
    "\n",
    "ALL_BUTTONS_ENCODED = get_all_buttons_encoded(QWERTY_ENCODED_HIGH, QWERTY_ENCODED_LOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogType = Literal[\"basic\"] | Literal[\"debug\"] | Literal[\"error\"]\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, verbose: bool = True, hide_types: list[LogType] = []) -> None:\n",
    "        self.verbose = verbose\n",
    "        self.hide_types = set(hide_types)\n",
    "\n",
    "    def log(self, message: str, log_type: LogType = \"basic\") -> None:\n",
    "        if self.verbose and log_type not in self.hide_types:\n",
    "            print(message)\n",
    "\n",
    "\n",
    "LOGGER = Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Position = tuple[int, int]\n",
    "\n",
    "\n",
    "class Finger:\n",
    "    def __init__(\n",
    "        self, initial_position: Position, name: str, logger: Logger = LOGGER\n",
    "    ) -> None:\n",
    "        self.name = name\n",
    "        self.initial_position = initial_position\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        # Constants\n",
    "\n",
    "        self.wait_before_return = 4  # in ticks\n",
    "\n",
    "        self.long_row_move_shift = 3\n",
    "        self.long_row_move_penalty = 1\n",
    "\n",
    "        self.row_penalty_coefficient = 1\n",
    "        self.column_penalty_coefficient = 1.2\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = self.initial_position\n",
    "        self.ticks_before_return = 0  # if == 0, returns to the initial position\n",
    "        self.typed_keys = 0\n",
    "\n",
    "    def move(self, position: Position):\n",
    "        self.current_position = position\n",
    "\n",
    "        self.ticks_before_return = self.wait_before_return\n",
    "        self.typed_keys += 1\n",
    "\n",
    "    def tick(self) -> float:\n",
    "        if self.ticks_before_return > 0:\n",
    "            self.ticks_before_return -= 1\n",
    "\n",
    "        if self.ticks_before_return == 0:\n",
    "            score = self.get_score(self.initial_position)\n",
    "            self.current_position = self.initial_position\n",
    "            return score\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def get_score(self, target_position: Position) -> float:\n",
    "        x1, y1 = self.current_position\n",
    "        x2, y2 = target_position\n",
    "\n",
    "        row_distance = abs(x1 - x2) ** 2\n",
    "        column_distance = abs(y1 - y2) ** 2\n",
    "\n",
    "        penalty = 0\n",
    "        if row_distance > self.long_row_move_shift:\n",
    "            penalty = self.long_row_move_penalty\n",
    "        return (\n",
    "            row_distance * self.row_penalty_coefficient\n",
    "            + column_distance * self.column_penalty_coefficient\n",
    "            + penalty\n",
    "        )\n",
    "\n",
    "    def show_statistics(self):\n",
    "        self.logger.log(\n",
    "            f\"Name: {self.name:22} \\\n",
    "            Typed keys: {self.typed_keys:5} \\\n",
    "            Ticks before return: {self.ticks_before_return:5} \\\n",
    "            Current position: {self.current_position}\\t\\\n",
    "            Default position: {self.initial_position}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FINGERS: list[Finger] = [\n",
    "    Finger((2, 1), \"левый мизинец\"),\n",
    "    Finger((2, 2), \"левый безымянный\"),\n",
    "    Finger((2, 3), \"левый средний\"),\n",
    "    Finger((2, 4), \"левый указательный\"),\n",
    "    Finger((4, 3), \"левый большой\"),\n",
    "    Finger((4, 6), \"правый большой\"),\n",
    "    Finger((2, 7), \"правый указательный\"),\n",
    "    Finger((2, 8), \"правый средний\"),\n",
    "    Finger((2, 9), \"правый безымянный\"),\n",
    "    Finger((2, 10), \"правый мизинец\"),\n",
    "]\n",
    "\n",
    "SwapType = Literal[\"low_layout\"] | Literal[\"high_layout\"] | Literal[\"between_layouts\"]\n",
    "\n",
    "\n",
    "class KeyboardLayout:\n",
    "    @staticmethod\n",
    "    def layout_to_dict(\n",
    "        layout: Layout, unused_layout: Layout\n",
    "    ) -> dict[int, list[Position]]:\n",
    "        layout_dict: dict[int, list[Position]] = {}\n",
    "\n",
    "        for i in range(len(layout)):\n",
    "            for j in range(len(layout[i])):\n",
    "                button = layout[i][j]\n",
    "                if button in layout_dict:\n",
    "                    layout_dict[button].append((i, j))\n",
    "                else:\n",
    "                    layout_dict[button] = [(i, j)]\n",
    "\n",
    "        for i in range(len(unused_layout)):\n",
    "            for j in range(len(unused_layout[i])):\n",
    "                button = unused_layout[i][j]\n",
    "                if button not in layout_dict:\n",
    "                    layout_dict[button] = []\n",
    "\n",
    "        return layout_dict\n",
    "\n",
    "    def _finish_move(self):\n",
    "        for finger in self.fingers:\n",
    "            self.total_score += finger.tick()\n",
    "\n",
    "    def __init__(self, low_layout: Layout, high_layout: Layout, logger: Logger = LOGGER):\n",
    "        self.low_layout = deepcopy(low_layout)\n",
    "        self.high_layout = deepcopy(high_layout)\n",
    "\n",
    "        self.low_layout_dict = KeyboardLayout.layout_to_dict(\n",
    "            self.low_layout, self.high_layout\n",
    "        )\n",
    "        self.high_layout_dict = KeyboardLayout.layout_to_dict(\n",
    "            self.high_layout, self.low_layout\n",
    "        )\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        self.fingers = deepcopy(DEFAULT_FINGERS)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_score: float = 0\n",
    "        self.typed_keys: int = 0\n",
    "        for f in self.fingers:\n",
    "            f.reset()\n",
    "\n",
    "    def move_one_finger(\n",
    "        self, positions: list[Position], busy_finger_id: Optional[int] = None\n",
    "    ) -> tuple[tuple[int, Position], float]:\n",
    "        best_finger_id: int = 0\n",
    "        best_score = np.inf\n",
    "\n",
    "        final_position: Position = (0, 0)\n",
    "\n",
    "        for position in positions:\n",
    "            scores = [\n",
    "                finger.get_score(position) if i != busy_finger_id else np.inf\n",
    "                for i, finger in enumerate(self.fingers)\n",
    "            ]\n",
    "\n",
    "            candidate_finger_id = int(np.argmin(scores))\n",
    "            candidate_score = scores[candidate_finger_id]\n",
    "\n",
    "            if candidate_score < best_score:\n",
    "                best_score = candidate_score\n",
    "                best_finger_id = candidate_finger_id\n",
    "                final_position = position\n",
    "\n",
    "        return (best_finger_id, final_position), best_score\n",
    "\n",
    "    def move_two_fingers(\n",
    "        self, positions: list[Position]\n",
    "    ) -> tuple[tuple[int, Position], tuple[int, Position], float]:\n",
    "        shift_positions = self.low_layout_dict[SHIFT_CODE]\n",
    "        if len(shift_positions) == 0:\n",
    "            print(\"ERROR SHIFT IS UNREACHABLE\")\n",
    "            return (0, (0, 0)), (0, (0, 0)), 9999\n",
    "\n",
    "        # firstly reach SHIFT, then - positions\n",
    "        finger_shift_info_1, shift_distance_1 = self.move_one_finger(shift_positions)\n",
    "        finger_btn_info_1, d1_btn = self.move_one_finger(\n",
    "            positions, finger_shift_info_1[0]\n",
    "        )\n",
    "        total_distance_1 = shift_distance_1 + d1_btn\n",
    "\n",
    "        # firstly reach positions, then - SHIFT\n",
    "        finger_btn_info_2, d1_btn = self.move_one_finger(positions)\n",
    "        finger_shift_info_2, shift_distance_2 = self.move_one_finger(\n",
    "            shift_positions, finger_btn_info_2[0]\n",
    "        )\n",
    "        total_distance_2 = shift_distance_2 + d1_btn\n",
    "\n",
    "        if total_distance_1 < total_distance_2:\n",
    "            return finger_btn_info_1, finger_shift_info_1, total_distance_1\n",
    "\n",
    "        return finger_btn_info_2, finger_shift_info_2, total_distance_2\n",
    "\n",
    "    def find_button(self, button: int):\n",
    "        if button in self.low_layout_dict and len(self.low_layout_dict[button]) > 0:\n",
    "            (finger_id, finger_position), score = self.move_one_finger(\n",
    "                self.low_layout_dict[button]\n",
    "            )\n",
    "\n",
    "            self.fingers[finger_id].move(finger_position)\n",
    "            self.total_score += score\n",
    "            self.typed_keys += 1\n",
    "\n",
    "            self.logger.log(f\"{button}:\\t{self.fingers[finger_id].name}\")\n",
    "\n",
    "        elif button in self.high_layout_dict and len(self.high_layout_dict[button]) > 0:\n",
    "            (\n",
    "                (finger_id_1, finger_position_1),\n",
    "                (finger_id_2, finger_position_2),\n",
    "                score,\n",
    "            ) = self.move_two_fingers(self.high_layout_dict[button])\n",
    "\n",
    "            self.fingers[finger_id_1].move(finger_position_1)\n",
    "            self.fingers[finger_id_2].move(finger_position_2)\n",
    "            self.total_score += score\n",
    "            self.typed_keys += 2\n",
    "\n",
    "            self.logger.log(\n",
    "                f\"{button}:\\t{self.fingers[finger_id_1].name} + {self.fingers[finger_id_2].name}\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.logger.log(f\"NO SUCH KEY: {button}\")\n",
    "\n",
    "        self._finish_move()\n",
    "\n",
    "    def type_text(self, text: list[str]) -> float:\n",
    "        for button in text:\n",
    "            self.find_button(ENCODE_DICT[button])\n",
    "        return self.total_score\n",
    "\n",
    "    def type_encoded_text(self, encoded_text: list[int]) -> float:\n",
    "        for button in encoded_text:\n",
    "            self.find_button(button)\n",
    "        return self.total_score\n",
    "\n",
    "    def swap_buttons(self, position1: Position, position2: Position, swap_type: SwapType):\n",
    "        if swap_type == \"high_layout\":\n",
    "            layout_from = layout_to = self.high_layout\n",
    "            layout_from_dict = layout_to_dict = self.high_layout_dict\n",
    "        elif swap_type == \"low_layout\":\n",
    "            layout_from = layout_to = self.low_layout\n",
    "            layout_from_dict = layout_to_dict = self.low_layout_dict\n",
    "        else:  # swap_type == \"between_layouts\"\n",
    "            layout_from = self.low_layout\n",
    "            layout_to = self.high_layout\n",
    "            layout_from_dict = self.low_layout_dict\n",
    "            layout_to_dict = self.high_layout_dict\n",
    "\n",
    "        x1, y1 = position1\n",
    "        btn1 = layout_from[x1][y1]\n",
    "        x2, y2 = position2\n",
    "        btn2 = layout_to[x2][y2]\n",
    "\n",
    "        layout_from[x1][y1], layout_to[x2][y2] = layout_to[x2][y2], layout_from[x1][y1]\n",
    "\n",
    "        layout_from_dict[btn1].remove(position1)\n",
    "        layout_to_dict[btn2].remove(position2)\n",
    "\n",
    "        layout_from_dict[btn2].append(position1)\n",
    "        layout_to_dict[btn1].append(position2)\n",
    "\n",
    "    def decode_layouts(self) -> tuple[list[list[str]], list[list[str]]]:\n",
    "        return (decode_layout(self.low_layout), decode_layout(self.high_layout))\n",
    "\n",
    "    def get_string_layouts(self) -> str:\n",
    "        low_layout, high_layout = self.decode_layouts()\n",
    "        result_string = \"High layout:\\n\"\n",
    "        for row in high_layout:\n",
    "            for s in row:\n",
    "                result_string += f\"{s:8}\"\n",
    "            result_string += \"\\n\"\n",
    "        result_string += \"\\n\"\n",
    "\n",
    "        result_string += \"\\nLow layout:\\n\"\n",
    "        for row in low_layout:\n",
    "            for s in row:\n",
    "                result_string += f\"{s:8}\"\n",
    "            result_string += \"\\n\"\n",
    "        result_string += \"\\n\"\n",
    "\n",
    "        return result_string\n",
    "\n",
    "    def show_statistics(self):\n",
    "        self.logger.log(\"\\nStatistics:\")\n",
    "        for f in self.fingers:\n",
    "            f.show_statistics()\n",
    "\n",
    "    def flatten(self):\n",
    "        flatten = []\n",
    "        for row in self.low_layout:\n",
    "            flatten.extend(row)\n",
    "        for row in self.high_layout:\n",
    "            flatten.extend(row)\n",
    "        # flatten = [flatten]\n",
    "        flatten = [x / len(BUTTONS_SET) for x in flatten]\n",
    "        return torch.as_tensor(flatten, dtype=torch.float32)\n",
    "\n",
    "    def get_average_score(self) -> float:\n",
    "        return self.total_score / self.typed_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>point operator-(point p1,point p2)\\r\\n  p1.x-=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>int main() {\\r\\n    map&lt;char, int&gt; m;\\r\\n    m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>int main()\\r\\n\\tint n,s[1001],cnt=0;\\r\\n\\tscan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>int main() {\\r\\n\\tcin &gt;&gt; n;\\r\\n\\tfor (int i = ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>int main()\\r\\n      char a, b, c, d, kozir;\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  point operator-(point p1,point p2)\\r\\n  p1.x-=...\n",
       "1  int main() {\\r\\n    map<char, int> m;\\r\\n    m...\n",
       "2  int main()\\r\\n\\tint n,s[1001],cnt=0;\\r\\n\\tscan...\n",
       "3  int main() {\\r\\n\\tcin >> n;\\r\\n\\tfor (int i = ...\n",
       "4    int main()\\r\\n      char a, b, c, d, kozir;\\..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv(\"../data/raw/cpp_programs.csv\", index_col=0)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 484.69309686721283\n",
      "Max length: 62514\n",
      "Min length: 13\n"
     ]
    }
   ],
   "source": [
    "text_lengths = data_frame[\"text\"].apply(len)\n",
    "print(f\"Mean length: {text_lengths.mean()}\")\n",
    "print(f\"Max length: {text_lengths.max()}\")\n",
    "print(f\"Min length: {text_lengths.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 400\n",
    "PADDING_VALUE = ENCODE_DICT[\"<space>\"]\n",
    "\n",
    "SPECIAL_SYMBOLS = {\n",
    "    \"\\t\": \"<tab>\",\n",
    "    \"\\n\": \"<enter>\",\n",
    "    \" \": \"<space>\",\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> list[int]:\n",
    "    splitted_text = [\n",
    "        SPECIAL_SYMBOLS[s] if s in SPECIAL_SYMBOLS else s for s in list(text)\n",
    "    ]\n",
    "    encoded_text = [\n",
    "        ENCODE_DICT[symbol]\n",
    "        for symbol in list(filter(lambda s: s in BUTTONS_SET, splitted_text))[:MAX_LENGTH]\n",
    "    ]\n",
    "    return encoded_text + [PADDING_VALUE for _ in range(MAX_LENGTH - len(encoded_text))]\n",
    "\n",
    "\n",
    "data_frame[\"encoded_text\"] = data_frame[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119989, 400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.array([np.array(text) for text in data_frame[\"encoded_text\"].to_list()])\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s, qwerty=2.49e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 34.72it/s, qwerty=1.03e+4]\n"
     ]
    }
   ],
   "source": [
    "qwerty_keyboard = KeyboardLayout(\n",
    "    QWERTY_ENCODED_LOW, QWERTY_ENCODED_HIGH, Logger(verbose=False)\n",
    ")\n",
    "\n",
    "loop = tqdm(dataset[:10])\n",
    "for text in loop:\n",
    "    qwerty_score_total = qwerty_keyboard.type_encoded_text(text)\n",
    "    loop.set_postfix({\"qwerty\": qwerty_score_total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_layout(layout: KeyboardLayout, dataset: np.ndarray) -> list[float]:\n",
    "    score = 0\n",
    "    loop = dataset\n",
    "    layout.reset()\n",
    "    for text in loop:\n",
    "        layout.type_encoded_text(text)\n",
    "\n",
    "    score = layout.total_score\n",
    "    # scores.append(layout.get_average_score())\n",
    "    return torch.tensor([score]).float().unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_to_cord(n: int) -> tuple[int, int, int]:\n",
    "    shift = 0\n",
    "    row = 0\n",
    "    column = 0\n",
    "    if n >= KEYS:\n",
    "        return 2, 0, 0\n",
    "    if n < 0:\n",
    "        return 2, 0, 0\n",
    "    if n >= KEYS // 2:\n",
    "        n -= KEYS // 2\n",
    "        shift = 1\n",
    "    for i in range(len(KEYBOARD_LAYOUT_CUMSUM_SHAPE)):\n",
    "        if n < KEYBOARD_LAYOUT_CUMSUM_SHAPE[i]:\n",
    "            row = i\n",
    "            break\n",
    "    if row > 0:\n",
    "        n -= KEYBOARD_LAYOUT_CUMSUM_SHAPE[row - 1]\n",
    "    column = n\n",
    "    return shift, row, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8218.080000000038"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwerty_score_total * 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor for future utilities\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# number of episodes to run\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "# max steps per episode\n",
    "MAX_STEPS = 100\n",
    "\n",
    "# score agent needs for environment to be solved\n",
    "SOLVED_SCORE = qwerty_score_total * 0.8\n",
    "\n",
    "# device to run model on\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a neural network to learn our policy parameters\n",
    "class PolicyNetwork(nn.Module):\n",
    "    # Takes in observations and outputs actions\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, action_space)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        # input states\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # relu activation\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # actions\n",
    "        actions = self.output_layer(x)\n",
    "\n",
    "        # get softmax for a probability distribution\n",
    "        action_probs = F.softmax(actions, dim=0)\n",
    "\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a neural network to learn state value\n",
    "class StateValueNetwork(nn.Module):\n",
    "    # Takes in state\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input layer\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # activiation relu\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # get state value\n",
    "        state_value = self.output_layer(x)\n",
    "\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(network, state):\n",
    "    \"\"\"Selects an action given current state\n",
    "    Args:\n",
    "    - network (Torch NN): network to process state\n",
    "    - state (Array): Array of action space in an environment\n",
    "\n",
    "    Return:\n",
    "    - (int): action that is selected\n",
    "    - (float): log probability of selecting that action given state and network\n",
    "    \"\"\"\n",
    "\n",
    "    # convert state to float tensor, add 1 dimension, allocate tensor on device\n",
    "    state = state.flatten().to(DEVICE)\n",
    "\n",
    "    # use network to predict action probabilities\n",
    "    action_probs = network(state)\n",
    "    # state = state.detach()\n",
    "    # print(action_probs)\n",
    "    # print(torch.sum(action_probs).item(), torch.argmax(action_probs).item(), action_probs[torch.argmax(action_probs).item()].item())\n",
    "\n",
    "    # sample an action using the probability distribution\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "\n",
    "    # return action\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make environment\n",
    "env = KeyboardLayout(QWERTY_ENCODED_LOW, QWERTY_ENCODED_HIGH, Logger(verbose=False))\n",
    "\n",
    "# Init network\n",
    "policy_network = PolicyNetwork(KEYS, ACTION_SPACE_SIZE).to(DEVICE)\n",
    "stateval_network = StateValueNetwork(KEYS).to(DEVICE)\n",
    "\n",
    "# Init optimizer\n",
    "policy_optimizer = optim.SGD(policy_network.parameters(), lr=0.001)\n",
    "stateval_optimizer = optim.SGD(stateval_network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = qwerty_score_total\n",
    "best_keyboard = env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions (4, 9) (0, 6) between_layouts\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.1788, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4903e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10585.7998046875 -313.19980468745234 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.0378, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2756, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10833.0 -560.3999999999523 -313.19980468745234\n",
      "lp, policy_loss tensor(-9.1688, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5002, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (4, 1) high_layout\n",
      "Score, reward, prev_reward 10847.7998046875 -575.1998046874523 -560.3999999999523\n",
      "lp, policy_loss tensor(-9.1797, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5140, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (3, 3) high_layout\n",
      "Score, reward, prev_reward 10847.7998046875 -575.1998046874523 -575.1998046874523\n",
      "lp, policy_loss tensor(-9.0740, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5081, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 10847.7998046875 -575.1998046874523 -575.1998046874523\n",
      "lp, policy_loss tensor(-9.0055, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5043, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 2) (4, 6) high_layout\n",
      "Score, reward, prev_reward 10847.7998046875 -575.1998046874523 -575.1998046874523\n",
      "lp, policy_loss tensor(-9.0285, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5055, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (4, 2) (3, 3) between_layouts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kadav\\AppData\\Local\\Temp\\ipykernel_24456\\2958338285.py:80: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.0644, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4468e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 8) (0, 9) between_layouts\n",
      "Score, reward, prev_reward 9846.7998046875 425.80019531254766 0.0003906250476575224\n",
      "lp, policy_loss tensor(-8.9767, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3721, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (3, 12) between_layouts\n",
      "Score, reward, prev_reward 9801.7998046875 470.80019531254766 425.80019531254766\n",
      "lp, policy_loss tensor(-8.8593, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4060, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 10) (2, 1) between_layouts\n",
      "Score, reward, prev_reward 9853.400390625 419.19960937504766 470.80019531254766\n",
      "lp, policy_loss tensor(-9.0514, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3694, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (2, 10) high_layout\n",
      "Score, reward, prev_reward 9853.400390625 419.19960937504766 419.19960937504766\n",
      "lp, policy_loss tensor(-9.0258, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3683, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (3, 9) high_layout\n",
      "Score, reward, prev_reward 10147.599609375 125.00039062504766 419.19960937504766\n",
      "lp, policy_loss tensor(-9.2106, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (3, 7) high_layout\n",
      "Score, reward, prev_reward 9962.0 310.60000000004766 125.00039062504766\n",
      "lp, policy_loss tensor(-9.1321, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2761, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 9907.0 365.60000000004766 310.60000000004766\n",
      "lp, policy_loss tensor(-9.1224, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3247, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (4, 6) high_layout\n",
      "Score, reward, prev_reward 9907.0 365.60000000004766 365.60000000004766\n",
      "lp, policy_loss tensor(-9.0651, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3226, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (4, 9) high_layout\n",
      "Score, reward, prev_reward 9907.0 365.60000000004766 365.60000000004766\n",
      "lp, policy_loss tensor(-9.0602, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3225, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 9880.400390625 392.19960937504766 365.60000000004766\n",
      "lp, policy_loss tensor(-9.4382, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3603, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10104.0 168.60000000004766 392.19960937504766\n",
      "lp, policy_loss tensor(-9.0778, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1490, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 10074.400390625 198.19960937504766 168.60000000004766\n",
      "lp, policy_loss tensor(-8.8711, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1712, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 13) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10074.400390625 198.19960937504766 198.19960937504766\n",
      "lp, policy_loss tensor(-9.2185, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1779, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (4, 10) (1, 1) between_layouts\n",
      "Score, reward, prev_reward 10300.599609375 -27.999609374952342 -10\n",
      "lp, policy_loss tensor(-8.9888, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0245, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 11) (4, 8) between_layouts\n",
      "Score, reward, prev_reward 10341.7998046875 -69.19980468745234 -27.999609374952342\n",
      "lp, policy_loss tensor(-9.3300, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0629, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (1, 11) high_layout\n",
      "Score, reward, prev_reward 10415.2001953125 -142.60019531245234 -69.19980468745234\n",
      "lp, policy_loss tensor(-9.2809, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1288, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (4, 7) low_layouts\n",
      "Score, reward, prev_reward 10415.2001953125 -142.60019531245234 -142.60019531245234\n",
      "lp, policy_loss tensor(-9.1797, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1274, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 9672.2001953125 600.3998046875477 -142.60019531245234\n",
      "lp, policy_loss tensor(-8.9167, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5212, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (1, 8) between_layouts\n",
      "Score, reward, prev_reward 9753.0 519.6000000000477 600.3998046875477\n",
      "lp, policy_loss tensor(-9.0761, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4591, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 10197.2001953125 75.39980468754766 519.6000000000477\n",
      "lp, policy_loss tensor(-8.8162, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0647, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (1, 3) high_layout\n",
      "Score, reward, prev_reward 10173.599609375 99.00039062504766 75.39980468754766\n",
      "lp, policy_loss tensor(-9.1665, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0883, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 3) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10174.400390625 98.19960937504766 99.00039062504766\n",
      "lp, policy_loss tensor(-9.1106, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0871, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (3, 12) low_layouts\n",
      "Score, reward, prev_reward 10187.400390625 85.19960937504766 98.19960937504766\n",
      "lp, policy_loss tensor(-9.1304, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0757, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 2) (3, 4) low_layouts\n",
      "Score, reward, prev_reward 10282.599609375 -9.999609374952342 -10\n",
      "lp, policy_loss tensor(-8.9070, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0087, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 5) high_layout\n",
      "Score, reward, prev_reward 10282.599609375 -9.999609374952342 -9.999609374952342\n",
      "lp, policy_loss tensor(-9.1034, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0089, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 3) (4, 2) low_layouts\n",
      "Score, reward, prev_reward 10307.7998046875 -35.19980468745234 -9.999609374952342\n",
      "lp, policy_loss tensor(-9.1082, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0312, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (2, 1) between_layouts\n",
      "Score, reward, prev_reward 10243.7998046875 28.800195312547658 -35.19980468745234\n",
      "lp, policy_loss tensor(-9.2041, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0258, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 10243.7998046875 28.800195312547658 28.800195312547658\n",
      "lp, policy_loss tensor(-9.0071, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (2, 11) high_layout\n",
      "Score, reward, prev_reward 10190.2001953125 82.39980468754766 28.800195312547658\n",
      "lp, policy_loss tensor(-9.2145, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0739, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (1, 8) between_layouts\n",
      "Score, reward, prev_reward 10054.400390625 218.19960937504766 82.39980468754766\n",
      "lp, policy_loss tensor(-9.0429, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1921, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 11) (2, 4) between_layouts\n",
      "Score, reward, prev_reward 10101.400390625 171.19960937504766 218.19960937504766\n",
      "lp, policy_loss tensor(-9.1027, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1517, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (4, 3) low_layouts\n",
      "Score, reward, prev_reward 10101.400390625 171.19960937504766 171.19960937504766\n",
      "lp, policy_loss tensor(-8.9714, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1495, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 10350.2001953125 -77.60019531245234 171.19960937504766\n",
      "lp, policy_loss tensor(-9.1756, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0693, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 10505.0 -232.39999999995234 -77.60019531245234\n",
      "lp, policy_loss tensor(-9.1310, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2066, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (1, 13) between_layouts\n",
      "Score, reward, prev_reward 10639.2001953125 -366.60019531245234 -232.39999999995234\n",
      "lp, policy_loss tensor(-9.1601, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3269, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 6) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 10639.2001953125 -366.60019531245234 -366.60019531245234\n",
      "lp, policy_loss tensor(-8.9939, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3210, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 9) (4, 0) high_layout\n",
      "Score, reward, prev_reward 10942.2001953125 -669.6001953124523 -10\n",
      "lp, policy_loss tensor(-9.0867, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5923, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (4, 9) between_layouts\n",
      "Score, reward, prev_reward 10926.7998046875 -654.1998046874523 -669.6001953124523\n",
      "lp, policy_loss tensor(-9.0763, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5780, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (1, 12) low_layouts\n",
      "Score, reward, prev_reward 10305.2001953125 -32.60019531245234 -654.1998046874523\n",
      "lp, policy_loss tensor(-9.0910, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0289, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (3, 12) low_layouts\n",
      "Score, reward, prev_reward 10131.599609375 141.00039062504766 -32.60019531245234\n",
      "lp, policy_loss tensor(-9.1007, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1249, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (3, 12) low_layouts\n",
      "Score, reward, prev_reward 10414.7998046875 -142.19980468745234 141.00039062504766\n",
      "lp, policy_loss tensor(-9.0865, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1258, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (4, 5) low_layouts\n",
      "Score, reward, prev_reward 10419.599609375 -146.99960937495234 -142.19980468745234\n",
      "lp, policy_loss tensor(-9.0267, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1292, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (0, 13) between_layouts\n",
      "Score, reward, prev_reward 10573.599609375 -300.99960937495234 -146.99960937495234\n",
      "lp, policy_loss tensor(-9.2379, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2707, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (4, 10) high_layout\n",
      "Score, reward, prev_reward 10573.599609375 -300.99960937495234 -300.99960937495234\n",
      "lp, policy_loss tensor(-8.8566, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2595, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (2, 2) between_layouts\n",
      "Score, reward, prev_reward 10630.400390625 -357.80039062495234 -300.99960937495234\n",
      "lp, policy_loss tensor(-9.0359, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3147, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (0, 1) between_layouts\n",
      "Score, reward, prev_reward 10655.2001953125 -382.60019531245234 -357.80039062495234\n",
      "lp, policy_loss tensor(-9.1496, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3408, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 6) (4, 9) between_layouts\n",
      "Score, reward, prev_reward 10501.7998046875 -229.19980468745234 -382.60019531245234\n",
      "lp, policy_loss tensor(-9.1493, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2041, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (2, 12) between_layouts\n",
      "Score, reward, prev_reward 9310.7998046875 961.8001953125477 -229.19980468745234\n",
      "lp, policy_loss tensor(-9.3564, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8760, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 8) high_layout\n",
      "Score, reward, prev_reward 9296.7998046875 975.8001953125477 961.8001953125477\n",
      "lp, policy_loss tensor(-8.9070, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8461, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (1, 1) between_layouts\n",
      "Score, reward, prev_reward 9386.400390625 886.1996093750477 975.8001953125477\n",
      "lp, policy_loss tensor(-9.1997, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7936, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (3, 2) high_layout\n",
      "Score, reward, prev_reward 9386.400390625 886.1996093750477 886.1996093750477\n",
      "lp, policy_loss tensor(-9.0519, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7809, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (3, 6) low_layouts\n",
      "Score, reward, prev_reward 9357.2001953125 915.3998046875477 886.1996093750477\n",
      "lp, policy_loss tensor(-9.0021, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8022, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (4, 8) between_layouts\n",
      "Score, reward, prev_reward 9357.2001953125 915.3998046875477 915.3998046875477\n",
      "lp, policy_loss tensor(-9.0035, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8023, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 9) (3, 0) high_layout\n",
      "Score, reward, prev_reward 10289.7998046875 -17.199804687452342 -10\n",
      "lp, policy_loss tensor(-9.2138, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0154, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 10) low_layouts\n",
      "Score, reward, prev_reward 12320.599609375 -2047.9996093749523 -17.199804687452342\n",
      "lp, policy_loss tensor(-8.9248, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.7793, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 2) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 10230.7998046875 41.80019531254766 -10\n",
      "lp, policy_loss tensor(-9.0489, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0368, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 10) (3, 3) between_layouts\n",
      "Score, reward, prev_reward 10230.7998046875 41.80019531254766 41.80019531254766\n",
      "lp, policy_loss tensor(-9.0250, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0367, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (3, 6) low_layouts\n",
      "Score, reward, prev_reward 10046.7998046875 225.80019531254766 41.80019531254766\n",
      "lp, policy_loss tensor(-9.0856, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1997, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 5) (4, 7) high_layout\n",
      "Score, reward, prev_reward 10046.7998046875 225.80019531254766 225.80019531254766\n",
      "lp, policy_loss tensor(-9.2683, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2037, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 9943.2001953125 329.39980468754766 225.80019531254766\n",
      "lp, policy_loss tensor(-9.1518, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2935, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (2, 7) between_layouts\n",
      "Score, reward, prev_reward 9968.400390625 304.19960937504766 329.39980468754766\n",
      "lp, policy_loss tensor(-8.8344, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2616, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (3, 9) between_layouts\n",
      "Score, reward, prev_reward 9979.599609375 293.00039062504766 304.19960937504766\n",
      "lp, policy_loss tensor(-8.9781, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2561, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (3, 7) high_layout\n",
      "Score, reward, prev_reward 9982.0 290.60000000004766 293.00039062504766\n",
      "lp, policy_loss tensor(-9.0357, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2556, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (1, 11) low_layouts\n",
      "Score, reward, prev_reward 10199.2001953125 73.39980468754766 290.60000000004766\n",
      "lp, policy_loss tensor(-9.1345, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0653, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 2) (4, 6) low_layouts\n",
      "Score, reward, prev_reward 10199.2001953125 73.39980468754766 73.39980468754766\n",
      "lp, policy_loss tensor(-9.1322, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0653, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (4, 0) (4, 6) high_layout\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.2798, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.5287e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 10228.2001953125 44.39980468754766 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.1561, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0396, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (3, 2) between_layouts\n",
      "Score, reward, prev_reward 10228.2001953125 44.39980468754766 44.39980468754766\n",
      "lp, policy_loss tensor(-9.0798, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0392, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 2) (3, 10) low_layouts\n",
      "Score, reward, prev_reward 10182.0 90.60000000004766 44.39980468754766\n",
      "lp, policy_loss tensor(-9.3120, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0821, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (3, 1) between_layouts\n",
      "Score, reward, prev_reward 10178.7998046875 93.80019531254766 90.60000000004766\n",
      "lp, policy_loss tensor(-9.0848, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0830, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 11053.400390625 -780.8003906249523 93.80019531254766\n",
      "lp, policy_loss tensor(-8.9292, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6787, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 11033.7998046875 -761.1998046874523 -780.8003906249523\n",
      "lp, policy_loss tensor(-9.1136, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6753, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 2) (2, 12) between_layouts\n",
      "Score, reward, prev_reward 9686.0 586.6000000000477 -761.1998046874523\n",
      "lp, policy_loss tensor(-9.2676, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5292, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 13) (4, 4) between_layouts\n",
      "Score, reward, prev_reward 9686.0 586.6000000000477 586.6000000000477\n",
      "lp, policy_loss tensor(-9.0337, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5159, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (2, 2) between_layouts\n",
      "Score, reward, prev_reward 9699.2001953125 573.3998046875477 586.6000000000477\n",
      "lp, policy_loss tensor(-9.0786, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5068, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 6) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 9656.599609375 616.0003906250477 573.3998046875477\n",
      "lp, policy_loss tensor(-8.8849, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5328, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (0, 9) between_layouts\n",
      "Score, reward, prev_reward 9572.599609375 700.0003906250477 616.0003906250477\n",
      "lp, policy_loss tensor(-8.8651, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6041, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (4, 0) between_layouts\n",
      "Score, reward, prev_reward 9646.599609375 626.0003906250477 700.0003906250477\n",
      "lp, policy_loss tensor(-9.0247, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5500, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 10) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 9664.2001953125 608.3998046875477 626.0003906250477\n",
      "lp, policy_loss tensor(-9.0827, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5379, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (2, 13) between_layouts\n",
      "Score, reward, prev_reward 9653.0 619.6000000000477 608.3998046875477\n",
      "lp, policy_loss tensor(-9.0276, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5445, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (2, 11) high_layout\n",
      "Score, reward, prev_reward 9641.400390625 631.1996093750477 619.6000000000477\n",
      "lp, policy_loss tensor(-9.0274, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5547, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 11) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 9672.400390625 600.1996093750477 631.1996093750477\n",
      "lp, policy_loss tensor(-9.0179, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5269, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (3, 2) high_layout\n",
      "Score, reward, prev_reward 9672.400390625 600.1996093750477 600.1996093750477\n",
      "lp, policy_loss tensor(-8.7948, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5139, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 10263.2001953125 9.399804687547658 600.1996093750477\n",
      "lp, policy_loss tensor(-9.0515, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0083, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (2, 11) high_layout\n",
      "Score, reward, prev_reward 10274.7998046875 -2.1998046874523425 9.399804687547658\n",
      "lp, policy_loss tensor(-9.0547, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0019, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (3, 2) between_layouts\n",
      "Score, reward, prev_reward 10274.7998046875 -2.1998046874523425 -2.1998046874523425\n",
      "lp, policy_loss tensor(-9.0921, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0019, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 6) (4, 10) high_layout\n",
      "Score, reward, prev_reward 10274.7998046875 -2.1998046874523425 -2.1998046874523425\n",
      "lp, policy_loss tensor(-9.0881, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0019, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (1, 4) between_layouts\n",
      "Score, reward, prev_reward 10317.0 -44.39999999995234 -2.1998046874523425\n",
      "lp, policy_loss tensor(-9.2055, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0398, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 6) (1, 6) between_layouts\n",
      "Score, reward, prev_reward 10436.2001953125 -163.60019531245234 -44.39999999995234\n",
      "lp, policy_loss tensor(-8.9795, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1430, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (4, 0) between_layouts\n",
      "Score, reward, prev_reward 10408.0 -135.39999999995234 -163.60019531245234\n",
      "lp, policy_loss tensor(-8.8507, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1167, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (4, 4) between_layouts\n",
      "Score, reward, prev_reward 9920.2001953125 352.39980468754766 -135.39999999995234\n",
      "lp, policy_loss tensor(-8.9859, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3083, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 9925.7998046875 346.80019531254766 352.39980468754766\n",
      "lp, policy_loss tensor(-9.0208, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3045, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (1, 6) high_layout\n",
      "Score, reward, prev_reward 10127.0 145.60000000004766 346.80019531254766\n",
      "lp, policy_loss tensor(-8.9782, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1273, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (2, 11) high_layout\n",
      "Score, reward, prev_reward 10096.2001953125 176.39980468754766 145.60000000004766\n",
      "lp, policy_loss tensor(-8.9212, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1532, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (3, 2) between_layouts\n",
      "Score, reward, prev_reward 10096.2001953125 176.39980468754766 176.39980468754766\n",
      "lp, policy_loss tensor(-8.8957, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1528, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (1, 3) high_layout\n",
      "Score, reward, prev_reward 10137.0 135.60000000004766 176.39980468754766\n",
      "lp, policy_loss tensor(-9.1275, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1205, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (0, 1) between_layouts\n",
      "Score, reward, prev_reward 10128.2001953125 144.39980468754766 135.60000000004766\n",
      "lp, policy_loss tensor(-9.1330, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1284, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (3, 3) high_layout\n",
      "Score, reward, prev_reward 10128.2001953125 144.39980468754766 144.39980468754766\n",
      "lp, policy_loss tensor(-8.9121, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (1, 11) low_layouts\n",
      "Score, reward, prev_reward 10083.2001953125 189.39980468754766 144.39980468754766\n",
      "lp, policy_loss tensor(-9.1286, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1683, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (4, 5) low_layouts\n",
      "Score, reward, prev_reward 9996.7998046875 275.80019531254766 189.39980468754766\n",
      "lp, policy_loss tensor(-9.1823, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2465, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 0) (3, 6) between_layouts\n",
      "Score, reward, prev_reward 9996.7998046875 275.80019531254766 275.80019531254766\n",
      "lp, policy_loss tensor(-9.1271, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2450, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 6) (2, 9) low_layouts\n",
      "Score, reward, prev_reward 9996.7998046875 275.80019531254766 275.80019531254766\n",
      "lp, policy_loss tensor(-9.1371, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2453, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (1, 10) between_layouts\n",
      "Score, reward, prev_reward 9996.7998046875 275.80019531254766 275.80019531254766\n",
      "lp, policy_loss tensor(-8.9619, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2406, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (0, 6) between_layouts\n",
      "Score, reward, prev_reward 9996.7998046875 275.80019531254766 275.80019531254766\n",
      "lp, policy_loss tensor(-9.2599, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2486, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (1, 10) between_layouts\n",
      "Score, reward, prev_reward 9979.2001953125 293.39980468754766 275.80019531254766\n",
      "lp, policy_loss tensor(-9.0016, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2571, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (2, 5) low_layouts\n",
      "Score, reward, prev_reward 10076.2001953125 196.39980468754766 293.39980468754766\n",
      "lp, policy_loss tensor(-8.9845, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1718, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (2, 3) low_layouts\n",
      "Score, reward, prev_reward 9677.2001953125 595.3998046875477 196.39980468754766\n",
      "lp, policy_loss tensor(-9.1731, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5317, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (4, 5) between_layouts\n",
      "Score, reward, prev_reward 9609.599609375 663.0003906250477 595.3998046875477\n",
      "lp, policy_loss tensor(-9.2277, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5956, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 10485.0 -212.39999999995234 663.0003906250477\n",
      "lp, policy_loss tensor(-9.1005, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1882, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (3, 8) high_layout\n",
      "Score, reward, prev_reward 10540.599609375 -267.99960937495234 -212.39999999995234\n",
      "lp, policy_loss tensor(-9.1070, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2376, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (3, 2) high_layout\n",
      "Score, reward, prev_reward 10540.599609375 -267.99960937495234 -267.99960937495234\n",
      "lp, policy_loss tensor(-9.1620, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2390, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 10545.7998046875 -273.19980468745234 -267.99960937495234\n",
      "lp, policy_loss tensor(-8.8318, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2349, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (1, 10) between_layouts\n",
      "Score, reward, prev_reward 11384.0 -1111.3999999999523 -273.19980468745234\n",
      "lp, policy_loss tensor(-9.0097, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.9748, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (2, 2) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 10472.7998046875 -200.19980468745234 -10\n",
      "lp, policy_loss tensor(-9.1141, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1776, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (1, 4) high_layout\n",
      "Score, reward, prev_reward 10249.400390625 23.199609375047658 -200.19980468745234\n",
      "lp, policy_loss tensor(-9.1570, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0207, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (3, 10) between_layouts\n",
      "Score, reward, prev_reward 10188.599609375 84.00039062504766 23.199609375047658\n",
      "lp, policy_loss tensor(-9.0287, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0738, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 10391.400390625 -118.80039062495234 84.00039062504766\n",
      "lp, policy_loss tensor(-9.1634, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1060, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10213.400390625 59.19960937504766 -118.80039062495234\n",
      "lp, policy_loss tensor(-9.4590, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0545, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (0, 11) between_layouts\n",
      "Score, reward, prev_reward 10133.0 139.60000000004766 59.19960937504766\n",
      "lp, policy_loss tensor(-9.0915, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1235, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (2, 12) low_layouts\n",
      "Score, reward, prev_reward 10208.599609375 64.00039062504766 139.60000000004766\n",
      "lp, policy_loss tensor(-9.2285, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0575, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 8) (0, 13) between_layouts\n",
      "Score, reward, prev_reward 10208.599609375 64.00039062504766 64.00039062504766\n",
      "lp, policy_loss tensor(-9.0346, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0563, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (2, 9) between_layouts\n",
      "Score, reward, prev_reward 10545.400390625 -272.80039062495234 64.00039062504766\n",
      "lp, policy_loss tensor(-8.9831, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2386, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (1, 0) between_layouts\n",
      "Score, reward, prev_reward 10391.7998046875 -119.19980468745234 -272.80039062495234\n",
      "lp, policy_loss tensor(-8.9102, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1034, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (4, 0) high_layout\n",
      "Score, reward, prev_reward 10391.7998046875 -119.19980468745234 -119.19980468745234\n",
      "lp, policy_loss tensor(-9.0046, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1045, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (3, 3) low_layouts\n",
      "Score, reward, prev_reward 10400.7998046875 -128.19980468745234 -119.19980468745234\n",
      "lp, policy_loss tensor(-8.9839, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 2) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10478.400390625 -205.80039062495234 -128.19980468745234\n",
      "lp, policy_loss tensor(-9.1131, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1826, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 11313.599609375 -1040.9996093749523 -205.80039062495234\n",
      "lp, policy_loss tensor(-9.2594, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.9383, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (2, 1) (3, 6) high_layout\n",
      "Score, reward, prev_reward 10271.0 1.6000000000476575 -10\n",
      "lp, policy_loss tensor(-9.1041, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (2, 12) high_layout\n",
      "Score, reward, prev_reward 10271.0 1.6000000000476575 1.6000000000476575\n",
      "lp, policy_loss tensor(-8.7801, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (3, 2) low_layouts\n",
      "Score, reward, prev_reward 9957.400390625 315.19960937504766 1.6000000000476575\n",
      "lp, policy_loss tensor(-9.1222, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2799, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (1, 11) high_layout\n",
      "Score, reward, prev_reward 10089.400390625 183.19960937504766 315.19960937504766\n",
      "lp, policy_loss tensor(-9.0624, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1616, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 11) (0, 3) between_layouts\n",
      "Score, reward, prev_reward 9266.0 1006.6000000000477 183.19960937504766\n",
      "lp, policy_loss tensor(-9.3993, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9210, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (4, 1) between_layouts\n",
      "Score, reward, prev_reward 9273.2001953125 999.3998046875477 1006.6000000000477\n",
      "lp, policy_loss tensor(-9.1202, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8873, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (3, 6) low_layouts\n",
      "Score, reward, prev_reward 9247.7998046875 1024.8001953125477 999.3998046875477\n",
      "lp, policy_loss tensor(-9.0275, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9006, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 9665.599609375 607.0003906250477 1024.8001953125477\n",
      "lp, policy_loss tensor(-9.2068, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5440, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (1, 12) low_layouts\n",
      "Score, reward, prev_reward 10209.2001953125 63.39980468754766 607.0003906250477\n",
      "lp, policy_loss tensor(-9.2005, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0568, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (2, 12) high_layout\n",
      "Score, reward, prev_reward 10209.2001953125 63.39980468754766 63.39980468754766\n",
      "lp, policy_loss tensor(-8.9502, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0552, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (1, 13) between_layouts\n",
      "Score, reward, prev_reward 10141.599609375 131.00039062504766 63.39980468754766\n",
      "lp, policy_loss tensor(-8.6734, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1106, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 6) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10146.2001953125 126.39980468754766 131.00039062504766\n",
      "lp, policy_loss tensor(-9.1011, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1120, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 11213.400390625 -940.8003906249523 126.39980468754766\n",
      "lp, policy_loss tensor(-8.9749, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8220, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 8) (3, 1) low_layouts\n",
      "Score, reward, prev_reward 11164.2001953125 -891.6001953124523 -940.8003906249523\n",
      "lp, policy_loss tensor(-9.3168, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8086, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (2, 6) between_layouts\n",
      "Score, reward, prev_reward 11154.2001953125 -881.6001953124523 -891.6001953124523\n",
      "lp, policy_loss tensor(-8.9297, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7664, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 5) (4, 6) between_layouts\n",
      "Score, reward, prev_reward 11115.599609375 -842.9996093749523 -881.6001953124523\n",
      "lp, policy_loss tensor(-9.1181, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7483, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (2, 2) low_layouts\n",
      "Score, reward, prev_reward 11022.599609375 -749.9996093749523 -842.9996093749523\n",
      "lp, policy_loss tensor(-9.1665, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6692, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (2, 11) between_layouts\n",
      "Score, reward, prev_reward 11059.7998046875 -787.1998046874523 -749.9996093749523\n",
      "lp, policy_loss tensor(-9.2971, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7124, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (4, 6) between_layouts\n",
      "Score, reward, prev_reward 11075.2001953125 -802.6001953124523 -787.1998046874523\n",
      "lp, policy_loss tensor(-9.0646, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7082, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 10974.7998046875 -702.1998046874523 -802.6001953124523\n",
      "lp, policy_loss tensor(-9.0652, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6197, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (3, 9) low_layouts\n",
      "Score, reward, prev_reward 11022.7998046875 -750.1998046874523 -702.1998046874523\n",
      "lp, policy_loss tensor(-9.0557, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6613, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 13) (1, 2) between_layouts\n",
      "Score, reward, prev_reward 11022.7998046875 -750.1998046874523 -750.1998046874523\n",
      "lp, policy_loss tensor(-9.2018, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6720, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (3, 9) between_layouts\n",
      "Score, reward, prev_reward 11275.599609375 -1002.9996093749523 -750.1998046874523\n",
      "lp, policy_loss tensor(-9.1479, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8932, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (3, 10) (4, 0) between_layouts\n",
      "Score, reward, prev_reward 10523.400390625 -250.80039062495234 -10\n",
      "lp, policy_loss tensor(-9.1150, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2225, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (3, 0) high_layout\n",
      "Score, reward, prev_reward 10523.400390625 -250.80039062495234 -250.80039062495234\n",
      "lp, policy_loss tensor(-9.1853, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2243, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (1, 2) low_layouts\n",
      "Score, reward, prev_reward 10170.599609375 102.00039062504766 -250.80039062495234\n",
      "lp, policy_loss tensor(-9.1321, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0907, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (2, 13) high_layout\n",
      "Score, reward, prev_reward 10170.599609375 102.00039062504766 102.00039062504766\n",
      "lp, policy_loss tensor(-9.1209, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0906, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 10116.599609375 156.00039062504766 102.00039062504766\n",
      "lp, policy_loss tensor(-9.0852, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1380, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (0, 10) between_layouts\n",
      "Score, reward, prev_reward 11355.2001953125 -1082.6001953124523 156.00039062504766\n",
      "lp, policy_loss tensor(-9.1579, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.9651, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (4, 0) (1, 10) between_layouts\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.1409, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4759e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 4) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 10489.599609375 -216.99960937495234 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.0551, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1913, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (3, 6) low_layouts\n",
      "Score, reward, prev_reward 10681.599609375 -408.99960937495234 -216.99960937495234\n",
      "lp, policy_loss tensor(-9.1646, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3649, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 10692.7998046875 -420.19980468745234 -408.99960937495234\n",
      "lp, policy_loss tensor(-9.2293, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3775, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (4, 5) low_layouts\n",
      "Score, reward, prev_reward 10621.2001953125 -348.60019531245234 -420.19980468745234\n",
      "lp, policy_loss tensor(-9.2262, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3131, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (4, 0) high_layout\n",
      "Score, reward, prev_reward 10668.400390625 -395.80039062495234 -348.60019531245234\n",
      "lp, policy_loss tensor(-8.8060, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3393, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (4, 1) between_layouts\n",
      "Score, reward, prev_reward 11229.2001953125 -956.6001953124523 -395.80039062495234\n",
      "lp, policy_loss tensor(-9.0478, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8425, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 9) (4, 6) between_layouts\n",
      "Score, reward, prev_reward 11218.400390625 -945.8003906249523 -956.6001953124523\n",
      "lp, policy_loss tensor(-9.0787, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8359, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (4, 0) high_layout\n",
      "Score, reward, prev_reward 11160.0 -887.3999999999523 -945.8003906249523\n",
      "lp, policy_loss tensor(-9.1716, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7923, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (2, 4) low_layouts\n",
      "Score, reward, prev_reward 10742.0 -469.39999999995234 -887.3999999999523\n",
      "lp, policy_loss tensor(-9.2179, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4212, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (2, 1) between_layouts\n",
      "Score, reward, prev_reward 10734.7998046875 -462.19980468745234 -469.39999999995234\n",
      "lp, policy_loss tensor(-9.0309, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4063, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (0, 12) between_layouts\n",
      "Score, reward, prev_reward 10779.599609375 -506.99960937495234 -462.19980468745234\n",
      "lp, policy_loss tensor(-9.0018, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4443, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (2, 1) high_layout\n",
      "Score, reward, prev_reward 10518.7998046875 -246.19980468745234 -506.99960937495234\n",
      "lp, policy_loss tensor(-9.2663, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2221, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (1, 1) high_layout\n",
      "Score, reward, prev_reward 10545.599609375 -272.99960937495234 -246.19980468745234\n",
      "lp, policy_loss tensor(-9.3297, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2479, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 5) (0, 13) between_layouts\n",
      "Score, reward, prev_reward 11136.400390625 -863.8003906249523 -272.99960937495234\n",
      "lp, policy_loss tensor(-8.8940, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7479, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (2, 1) low_layouts\n",
      "Score, reward, prev_reward 11011.400390625 -738.8003906249523 -863.8003906249523\n",
      "lp, policy_loss tensor(-9.1262, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6564, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (4, 10) high_layout\n",
      "Score, reward, prev_reward 11011.400390625 -738.8003906249523 -738.8003906249523\n",
      "lp, policy_loss tensor(-9.1797, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6602, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 8) (2, 11) between_layouts\n",
      "Score, reward, prev_reward 11105.400390625 -832.8003906249523 -738.8003906249523\n",
      "lp, policy_loss tensor(-8.7202, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7069, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (1, 12) low_layouts\n",
      "Score, reward, prev_reward 11601.400390625 -1328.8003906249523 -832.8003906249523\n",
      "lp, policy_loss tensor(-8.9326, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.1555, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (2, 13) (3, 4) between_layouts\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.0341, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4353e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (3, 13) between_layouts\n",
      "Score, reward, prev_reward 10238.7998046875 33.80019531254766 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.2683, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0305, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (4, 9) between_layouts\n",
      "Score, reward, prev_reward 10619.0 -346.39999999995234 33.80019531254766\n",
      "lp, policy_loss tensor(-9.3558, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3155, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 9) (1, 0) between_layouts\n",
      "Score, reward, prev_reward 10603.0 -330.39999999995234 -346.39999999995234\n",
      "lp, policy_loss tensor(-8.9521, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2879, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (1, 8) between_layouts\n",
      "Score, reward, prev_reward 10638.0 -365.39999999995234 -330.39999999995234\n",
      "lp, policy_loss tensor(-9.2248, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3281, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (0, 6) between_layouts\n",
      "Score, reward, prev_reward 10638.0 -365.39999999995234 -365.39999999995234\n",
      "lp, policy_loss tensor(-8.7674, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3119, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 10) (1, 12) high_layout\n",
      "Score, reward, prev_reward 10638.0 -365.39999999995234 -365.39999999995234\n",
      "lp, policy_loss tensor(-9.0354, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3214, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (3, 8) low_layouts\n",
      "Score, reward, prev_reward 10638.0 -365.39999999995234 -365.39999999995234\n",
      "lp, policy_loss tensor(-9.1712, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3262, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (4, 2) low_layouts\n",
      "Score, reward, prev_reward 13163.0 -2890.3999999999523 -365.39999999995234\n",
      "lp, policy_loss tensor(-9.0388, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-2.5432, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 2) (0, 4) between_layouts\n",
      "Score, reward, prev_reward 10289.2001953125 -16.600195312452342 -10\n",
      "lp, policy_loss tensor(-9.0628, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0146, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (3, 12) high_layout\n",
      "Score, reward, prev_reward 10507.400390625 -234.80039062495234 -16.600195312452342\n",
      "lp, policy_loss tensor(-9.2714, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2119, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (1, 1) low_layouts\n",
      "Score, reward, prev_reward 10508.0 -235.39999999995234 -234.80039062495234\n",
      "lp, policy_loss tensor(-8.9594, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2053, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (1, 8) low_layouts\n",
      "Score, reward, prev_reward 10508.0 -235.39999999995234 -235.39999999995234\n",
      "lp, policy_loss tensor(-9.0713, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2079, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (3, 3) high_layout\n",
      "Score, reward, prev_reward 10508.0 -235.39999999995234 -235.39999999995234\n",
      "lp, policy_loss tensor(-9.0223, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2067, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 5) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 10544.7998046875 -272.19980468745234 -235.39999999995234\n",
      "lp, policy_loss tensor(-8.9102, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2361, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (0, 4) between_layouts\n",
      "Score, reward, prev_reward 10485.599609375 -212.99960937495234 -272.19980468745234\n",
      "lp, policy_loss tensor(-9.2530, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1919, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (2, 3) between_layouts\n",
      "Score, reward, prev_reward 10460.2001953125 -187.60019531245234 -212.99960937495234\n",
      "lp, policy_loss tensor(-8.7409, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1596, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (2, 0) between_layouts\n",
      "Score, reward, prev_reward 10619.400390625 -346.80039062495234 -187.60019531245234\n",
      "lp, policy_loss tensor(-8.9269, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3014, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (4, 2) low_layouts\n",
      "Score, reward, prev_reward 10637.7998046875 -365.19980468745234 -346.80039062495234\n",
      "lp, policy_loss tensor(-8.9065, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3166, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (3, 0) low_layouts\n",
      "Score, reward, prev_reward 9999.2001953125 273.39980468754766 -365.19980468745234\n",
      "lp, policy_loss tensor(-9.1057, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2423, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (1, 10) between_layouts\n",
      "Score, reward, prev_reward 9999.2001953125 273.39980468754766 273.39980468754766\n",
      "lp, policy_loss tensor(-8.9460, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2381, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (1, 8) low_layouts\n",
      "Score, reward, prev_reward 9917.400390625 355.19960937504766 273.39980468754766\n",
      "lp, policy_loss tensor(-9.0104, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3116, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 9750.400390625 522.1996093750477 355.19960937504766\n",
      "lp, policy_loss tensor(-9.1351, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4644, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (1, 3) between_layouts\n",
      "Score, reward, prev_reward 9823.599609375 449.00039062504766 522.1996093750477\n",
      "lp, policy_loss tensor(-9.3165, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4072, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (0, 2) between_layouts\n",
      "Score, reward, prev_reward 9823.599609375 449.00039062504766 449.00039062504766\n",
      "lp, policy_loss tensor(-9.0844, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3971, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (4, 4) low_layouts\n",
      "Score, reward, prev_reward 9823.599609375 449.00039062504766 449.00039062504766\n",
      "lp, policy_loss tensor(-9.1491, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3999, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (1, 13) between_layouts\n",
      "Score, reward, prev_reward 9801.400390625 471.19960937504766 449.00039062504766\n",
      "lp, policy_loss tensor(-9.0384, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4146, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (3, 6) between_layouts\n",
      "Score, reward, prev_reward 9799.400390625 473.19960937504766 471.19960937504766\n",
      "lp, policy_loss tensor(-9.2146, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4245, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (4, 4) high_layout\n",
      "Score, reward, prev_reward 9799.400390625 473.19960937504766 473.19960937504766\n",
      "lp, policy_loss tensor(-9.3354, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4300, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (0, 3) between_layouts\n",
      "Score, reward, prev_reward 9799.400390625 473.19960937504766 473.19960937504766\n",
      "lp, policy_loss tensor(-9.2183, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4246, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (0, 10) between_layouts\n",
      "Score, reward, prev_reward 9631.0 641.6000000000477 473.19960937504766\n",
      "lp, policy_loss tensor(-8.6598, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5409, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (2, 7) low_layouts\n",
      "Score, reward, prev_reward 9587.0 685.6000000000477 641.6000000000477\n",
      "lp, policy_loss tensor(-8.9948, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6003, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (4, 5) between_layouts\n",
      "Score, reward, prev_reward 9467.599609375 805.0003906250477 685.6000000000477\n",
      "lp, policy_loss tensor(-9.0807, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7116, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 9467.599609375 805.0003906250477 805.0003906250477\n",
      "lp, policy_loss tensor(-9.1484, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7169, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (2, 3) high_layout\n",
      "Score, reward, prev_reward 10021.0 251.60000000004766 805.0003906250477\n",
      "lp, policy_loss tensor(-8.8323, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2163, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 11) (3, 3) high_layout\n",
      "Score, reward, prev_reward 10044.2001953125 228.39980468754766 251.60000000004766\n",
      "lp, policy_loss tensor(-9.2159, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2049, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 4) (2, 2) between_layouts\n",
      "Score, reward, prev_reward 10053.7998046875 218.80019531254766 228.39980468754766\n",
      "lp, policy_loss tensor(-9.0846, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1935, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (2, 2) between_layouts\n",
      "Score, reward, prev_reward 9927.599609375 345.00039062504766 218.80019531254766\n",
      "lp, policy_loss tensor(-9.2503, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3107, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (4, 8) high_layout\n",
      "Score, reward, prev_reward 9985.599609375 287.00039062504766 345.00039062504766\n",
      "lp, policy_loss tensor(-9.1517, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2557, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 4) (2, 12) between_layouts\n",
      "Score, reward, prev_reward 9015.7998046875 1256.8001953125477 287.00039062504766\n",
      "lp, policy_loss tensor(-9.1389, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.1181, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 4) high_layout\n",
      "Score, reward, prev_reward 9015.7998046875 1256.8001953125477 1256.8001953125477\n",
      "lp, policy_loss tensor(-8.9719, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.0977, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (2, 10) high_layout\n",
      "Score, reward, prev_reward 9014.400390625 1258.1996093750477 1256.8001953125477\n",
      "lp, policy_loss tensor(-8.9286, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.0936, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (3, 5) low_layouts\n",
      "Score, reward, prev_reward 9264.400390625 1008.1996093750477 1258.1996093750477\n",
      "lp, policy_loss tensor(-9.1578, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8988, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (2, 10) between_layouts\n",
      "Score, reward, prev_reward 9262.0 1010.6000000000477 1008.1996093750477\n",
      "lp, policy_loss tensor(-8.8422, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8699, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (3, 13) high_layout\n",
      "Score, reward, prev_reward 9738.599609375 534.0003906250477 1010.6000000000477\n",
      "lp, policy_loss tensor(-9.0215, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4690, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 3) low_layouts\n",
      "Score, reward, prev_reward 9608.599609375 664.0003906250477 534.0003906250477\n",
      "lp, policy_loss tensor(-9.2081, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5952, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 9608.599609375 664.0003906250477 664.0003906250477\n",
      "lp, policy_loss tensor(-9.1821, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5935, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 9611.599609375 661.0003906250477 664.0003906250477\n",
      "lp, policy_loss tensor(-9.0063, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5795, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 9569.2001953125 703.3998046875477 661.0003906250477\n",
      "lp, policy_loss tensor(-9.3002, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6368, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (2, 4) low_layouts\n",
      "Score, reward, prev_reward 9683.599609375 589.0003906250477 703.3998046875477\n",
      "lp, policy_loss tensor(-9.1751, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5261, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (3, 6) between_layouts\n",
      "Score, reward, prev_reward 9688.0 584.6000000000477 589.0003906250477\n",
      "lp, policy_loss tensor(-9.2251, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5250, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (1, 10) low_layouts\n",
      "Score, reward, prev_reward 9600.7998046875 671.8001953125477 584.6000000000477\n",
      "lp, policy_loss tensor(-9.0253, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5902, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (3, 12) high_layout\n",
      "Score, reward, prev_reward 9621.400390625 651.1996093750477 671.8001953125477\n",
      "lp, policy_loss tensor(-9.1178, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5780, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (3, 12) high_layout\n",
      "Score, reward, prev_reward 9884.2001953125 388.39980468754766 651.1996093750477\n",
      "lp, policy_loss tensor(-9.3824, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3547, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 9856.599609375 416.00039062504766 388.39980468754766\n",
      "lp, policy_loss tensor(-9.3147, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3772, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (3, 6) low_layouts\n",
      "Score, reward, prev_reward 9856.599609375 416.00039062504766 416.00039062504766\n",
      "lp, policy_loss tensor(-9.0485, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3664, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 6) (4, 6) high_layout\n",
      "Score, reward, prev_reward 9856.599609375 416.00039062504766 416.00039062504766\n",
      "lp, policy_loss tensor(-9.1746, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3715, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (1, 13) between_layouts\n",
      "Score, reward, prev_reward 9887.599609375 385.00039062504766 416.00039062504766\n",
      "lp, policy_loss tensor(-8.9906, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3370, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (3, 0) low_layouts\n",
      "Score, reward, prev_reward 9897.599609375 375.00039062504766 385.00039062504766\n",
      "lp, policy_loss tensor(-9.1427, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3338, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (2, 6) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 10282.2001953125 -9.600195312452342 -10\n",
      "lp, policy_loss tensor(-8.9178, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0083, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 10254.7998046875 17.800195312547658 -9.600195312452342\n",
      "lp, policy_loss tensor(-8.9578, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0155, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 10254.7998046875 17.800195312547658 17.800195312547658\n",
      "lp, policy_loss tensor(-9.1070, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0158, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 10395.2001953125 -122.60019531245234 17.800195312547658\n",
      "lp, policy_loss tensor(-9.1573, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1093, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 8) (1, 8) high_layout\n",
      "Score, reward, prev_reward 10326.7998046875 -54.19980468745234 -122.60019531245234\n",
      "lp, policy_loss tensor(-8.9264, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0471, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (3, 12) low_layouts\n",
      "Score, reward, prev_reward 10449.400390625 -176.80039062495234 -54.19980468745234\n",
      "lp, policy_loss tensor(-9.0456, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1557, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (3, 12) between_layouts\n",
      "Score, reward, prev_reward 9805.0 467.60000000004766 -176.80039062495234\n",
      "lp, policy_loss tensor(-9.0601, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4124, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (0, 4) between_layouts\n",
      "Score, reward, prev_reward 9805.0 467.60000000004766 467.60000000004766\n",
      "lp, policy_loss tensor(-9.0303, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4111, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 8) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 9989.400390625 283.19960937504766 467.60000000004766\n",
      "lp, policy_loss tensor(-9.1054, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2510, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (3, 0) low_layouts\n",
      "Score, reward, prev_reward 9989.400390625 283.19960937504766 283.19960937504766\n",
      "lp, policy_loss tensor(-8.9736, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2474, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 7) (2, 12) low_layouts\n",
      "Score, reward, prev_reward 9969.2001953125 303.39980468754766 -10\n",
      "lp, policy_loss tensor(-9.2979, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2746, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (3, 9) between_layouts\n",
      "Score, reward, prev_reward 9929.599609375 343.00039062504766 303.39980468754766\n",
      "lp, policy_loss tensor(-9.0911, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3035, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (0, 2) between_layouts\n",
      "Score, reward, prev_reward 10414.2001953125 -141.60019531245234 343.00039062504766\n",
      "lp, policy_loss tensor(-8.9517, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1234, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (4, 7) low_layouts\n",
      "Score, reward, prev_reward 10498.599609375 -225.99960937495234 -141.60019531245234\n",
      "lp, policy_loss tensor(-8.9135, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1961, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (4, 9) between_layouts\n",
      "Score, reward, prev_reward 10563.2001953125 -290.60019531245234 -225.99960937495234\n",
      "lp, policy_loss tensor(-8.9541, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2533, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 10584.0 -311.39999999995234 -290.60019531245234\n",
      "lp, policy_loss tensor(-8.8879, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2694, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 10580.0 -307.39999999995234 -311.39999999995234\n",
      "lp, policy_loss tensor(-9.1988, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2753, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (3, 0) high_layout\n",
      "Score, reward, prev_reward 10719.2001953125 -446.60019531245234 -307.39999999995234\n",
      "lp, policy_loss tensor(-8.9533, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3892, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 10931.2001953125 -658.6001953124523 -446.60019531245234\n",
      "lp, policy_loss tensor(-9.0560, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5806, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (4, 3) between_layouts\n",
      "Score, reward, prev_reward 10931.2001953125 -658.6001953124523 -658.6001953124523\n",
      "lp, policy_loss tensor(-8.8896, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5699, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 0) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 10138.0 134.60000000004766 -10\n",
      "lp, policy_loss tensor(-9.1708, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1202, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (3, 8) low_layouts\n",
      "Score, reward, prev_reward 10120.0 152.60000000004766 134.60000000004766\n",
      "lp, policy_loss tensor(-9.1713, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1362, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 2) (3, 2) between_layouts\n",
      "Score, reward, prev_reward 10132.7998046875 139.80019531254766 152.60000000004766\n",
      "lp, policy_loss tensor(-8.9324, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1216, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10136.2001953125 136.39980468754766 139.80019531254766\n",
      "lp, policy_loss tensor(-8.9879, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1193, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 10267.0 5.6000000000476575 136.39980468754766\n",
      "lp, policy_loss tensor(-8.9557, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0049, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10245.400390625 27.199609375047658 5.6000000000476575\n",
      "lp, policy_loss tensor(-9.0555, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0240, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (1, 8) low_layouts\n",
      "Score, reward, prev_reward 10166.7998046875 105.80019531254766 27.199609375047658\n",
      "lp, policy_loss tensor(-9.0864, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0936, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 7) (0, 13) between_layouts\n",
      "Score, reward, prev_reward 10195.599609375 77.00039062504766 105.80019531254766\n",
      "lp, policy_loss tensor(-9.1088, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0683, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 5) (4, 0) between_layouts\n",
      "Score, reward, prev_reward 10195.599609375 77.00039062504766 77.00039062504766\n",
      "lp, policy_loss tensor(-9.1353, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0685, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 10314.599609375 -41.99960937495234 77.00039062504766\n",
      "lp, policy_loss tensor(-8.7790, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0359, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 9) (4, 1) between_layouts\n",
      "Score, reward, prev_reward 10314.599609375 -41.99960937495234 -41.99960937495234\n",
      "lp, policy_loss tensor(-9.2921, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0380, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (2, 2) (4, 7) high_layout\n",
      "Score, reward, prev_reward 10282.2001953125 -9.600195312452342 -10\n",
      "lp, policy_loss tensor(-8.8469, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0083, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (3, 12) low_layouts\n",
      "Score, reward, prev_reward 10255.400390625 17.199609375047658 -9.600195312452342\n",
      "lp, policy_loss tensor(-9.0948, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0152, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (1, 10) low_layouts\n",
      "Score, reward, prev_reward 10254.7998046875 17.800195312547658 17.199609375047658\n",
      "lp, policy_loss tensor(-8.8629, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0154, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (4, 6) between_layouts\n",
      "Score, reward, prev_reward 9620.0 652.6000000000477 17.800195312547658\n",
      "lp, policy_loss tensor(-9.2933, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5904, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (2, 9) between_layouts\n",
      "Score, reward, prev_reward 9494.599609375 778.0003906250477 652.6000000000477\n",
      "lp, policy_loss tensor(-9.3466, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7079, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 9464.2001953125 808.3998046875477 778.0003906250477\n",
      "lp, policy_loss tensor(-9.0436, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7117, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (4, 4) high_layout\n",
      "Score, reward, prev_reward 9464.2001953125 808.3998046875477 808.3998046875477\n",
      "lp, policy_loss tensor(-9.0542, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7125, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (0, 11) between_layouts\n",
      "Score, reward, prev_reward 9464.2001953125 808.3998046875477 808.3998046875477\n",
      "lp, policy_loss tensor(-8.9413, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7036, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 9461.400390625 811.1996093750477 808.3998046875477\n",
      "lp, policy_loss tensor(-9.0176, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (2, 6) high_layout\n",
      "Score, reward, prev_reward 9461.400390625 811.1996093750477 811.1996093750477\n",
      "lp, policy_loss tensor(-9.0125, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7117, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (1, 3) between_layouts\n",
      "Score, reward, prev_reward 9492.400390625 780.1996093750477 811.1996093750477\n",
      "lp, policy_loss tensor(-8.8703, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6737, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 9582.599609375 690.0003906250477 780.1996093750477\n",
      "lp, policy_loss tensor(-8.9314, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5999, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (4, 9) between_layouts\n",
      "Score, reward, prev_reward 9723.7998046875 548.8001953125477 690.0003906250477\n",
      "lp, policy_loss tensor(-9.3307, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4985, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (1, 0) high_layout\n",
      "Score, reward, prev_reward 9723.7998046875 548.8001953125477 548.8001953125477\n",
      "lp, policy_loss tensor(-8.9392, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4776, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (4, 3) between_layouts\n",
      "Score, reward, prev_reward 9692.599609375 580.0003906250477 548.8001953125477\n",
      "lp, policy_loss tensor(-9.2553, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5226, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 9659.599609375 613.0003906250477 580.0003906250477\n",
      "lp, policy_loss tensor(-8.9904, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5365, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 7) (4, 1) between_layouts\n",
      "Score, reward, prev_reward 9659.599609375 613.0003906250477 613.0003906250477\n",
      "lp, policy_loss tensor(-9.0053, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5374, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (1, 6) between_layouts\n",
      "Score, reward, prev_reward 11173.400390625 -900.8003906249523 613.0003906250477\n",
      "lp, policy_loss tensor(-8.9949, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7888, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (0, 2) between_layouts\n",
      "Score, reward, prev_reward 11724.599609375 -1451.9996093749523 -900.8003906249523\n",
      "lp, policy_loss tensor(-8.9707, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.2680, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 12) (3, 8) low_layouts\n",
      "Score, reward, prev_reward 10605.400390625 -332.80039062495234 -10\n",
      "lp, policy_loss tensor(-9.1199, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2955, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 4) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 10605.400390625 -332.80039062495234 -332.80039062495234\n",
      "lp, policy_loss tensor(-8.9912, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2913, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 10) (2, 1) high_layout\n",
      "Score, reward, prev_reward 10273.2001953125 -0.6001953124523425 -10\n",
      "lp, policy_loss tensor(-9.1979, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 10273.2001953125 -0.6001953124523425 -0.6001953124523425\n",
      "lp, policy_loss tensor(-8.9266, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (0, 3) between_layouts\n",
      "Score, reward, prev_reward 10273.2001953125 -0.6001953124523425 -0.6001953124523425\n",
      "lp, policy_loss tensor(-8.9921, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 3) (0, 5) between_layouts\n",
      "Score, reward, prev_reward 10162.400390625 110.19960937504766 -0.6001953124523425\n",
      "lp, policy_loss tensor(-9.1904, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0986, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (3, 13) high_layout\n",
      "Score, reward, prev_reward 10189.0 83.60000000004766 110.19960937504766\n",
      "lp, policy_loss tensor(-8.8625, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0721, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (2, 12) between_layouts\n",
      "Score, reward, prev_reward 9034.2001953125 1238.3998046875477 83.60000000004766\n",
      "lp, policy_loss tensor(-8.9186, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.0752, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 2) (3, 9) between_layouts\n",
      "Score, reward, prev_reward 9011.599609375 1261.0003906250477 1238.3998046875477\n",
      "lp, policy_loss tensor(-8.7725, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.0769, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (3, 11) high_layout\n",
      "Score, reward, prev_reward 9011.599609375 1261.0003906250477 1261.0003906250477\n",
      "lp, policy_loss tensor(-9.1565, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.1240, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 9141.0 1131.6000000000477 1261.0003906250477\n",
      "lp, policy_loss tensor(-9.0329, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9950, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (1, 8) low_layouts\n",
      "Score, reward, prev_reward 9103.400390625 1169.1996093750477 1131.6000000000477\n",
      "lp, policy_loss tensor(-8.9000, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.0130, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (2, 2) between_layouts\n",
      "Score, reward, prev_reward 9111.400390625 1161.1996093750477 1169.1996093750477\n",
      "lp, policy_loss tensor(-9.0496, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.0230, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (0, 2) between_layouts\n",
      "Score, reward, prev_reward 9497.400390625 775.1996093750477 1161.1996093750477\n",
      "lp, policy_loss tensor(-8.8464, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6676, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 9487.400390625 785.1996093750477 775.1996093750477\n",
      "lp, policy_loss tensor(-9.2597, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7078, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (0, 1) between_layouts\n",
      "Score, reward, prev_reward 9547.400390625 725.1996093750477 785.1996093750477\n",
      "lp, policy_loss tensor(-9.1616, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6468, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (3, 3) between_layouts\n",
      "Score, reward, prev_reward 9547.400390625 725.1996093750477 725.1996093750477\n",
      "lp, policy_loss tensor(-9.0230, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6370, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (3, 1) low_layouts\n",
      "Score, reward, prev_reward 9592.0 680.6000000000477 725.1996093750477\n",
      "lp, policy_loss tensor(-9.0263, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5980, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (1, 8) between_layouts\n",
      "Score, reward, prev_reward 9576.2001953125 696.3998046875477 680.6000000000477\n",
      "lp, policy_loss tensor(-9.1337, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6192, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (2, 9) high_layout\n",
      "Score, reward, prev_reward 9576.2001953125 696.3998046875477 696.3998046875477\n",
      "lp, policy_loss tensor(-9.1022, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6171, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (3, 2) high_layout\n",
      "Score, reward, prev_reward 9576.2001953125 696.3998046875477 696.3998046875477\n",
      "lp, policy_loss tensor(-9.1578, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6208, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (3, 13) high_layout\n",
      "Score, reward, prev_reward 10160.400390625 112.19960937504766 696.3998046875477\n",
      "lp, policy_loss tensor(-8.9086, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0973, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 10271.7998046875 0.8001953125476575 112.19960937504766\n",
      "lp, policy_loss tensor(-9.0702, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (3, 2) between_layouts\n",
      "Score, reward, prev_reward 10151.0 121.60000000004766 0.8001953125476575\n",
      "lp, policy_loss tensor(-9.1084, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1078, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (4, 8) low_layouts\n",
      "Score, reward, prev_reward 10739.400390625 -466.80039062495234 121.60000000004766\n",
      "lp, policy_loss tensor(-8.9841, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4082, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10646.7998046875 -374.19980468745234 -466.80039062495234\n",
      "lp, policy_loss tensor(-8.9289, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (4, 0) high_layout\n",
      "Score, reward, prev_reward 10691.400390625 -418.80039062495234 -374.19980468745234\n",
      "lp, policy_loss tensor(-9.0250, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3679, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (3, 8) high_layout\n",
      "Score, reward, prev_reward 10691.400390625 -418.80039062495234 -418.80039062495234\n",
      "lp, policy_loss tensor(-8.9930, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3666, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (4, 0) high_layout\n",
      "Score, reward, prev_reward 11208.7998046875 -936.1998046874523 -418.80039062495234\n",
      "lp, policy_loss tensor(-9.1500, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8339, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (1, 1) low_layouts\n",
      "Score, reward, prev_reward 10566.7998046875 -294.19980468745234 -936.1998046874523\n",
      "lp, policy_loss tensor(-8.9543, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2564, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (4, 3) low_layouts\n",
      "Score, reward, prev_reward 10566.7998046875 -294.19980468745234 -294.19980468745234\n",
      "lp, policy_loss tensor(-9.3052, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2665, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (3, 11) high_layout\n",
      "Score, reward, prev_reward 10557.2001953125 -284.60019531245234 -294.19980468745234\n",
      "lp, policy_loss tensor(-9.0832, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2516, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (4, 10) high_layout\n",
      "Score, reward, prev_reward 10557.2001953125 -284.60019531245234 -284.60019531245234\n",
      "lp, policy_loss tensor(-9.1612, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2538, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 0) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 10592.400390625 -319.80039062495234 -284.60019531245234\n",
      "lp, policy_loss tensor(-9.1590, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2851, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (4, 4) between_layouts\n",
      "Score, reward, prev_reward 10629.2001953125 -356.60019531245234 -319.80039062495234\n",
      "lp, policy_loss tensor(-9.0531, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3143, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (3, 5) high_layout\n",
      "Score, reward, prev_reward 10582.7998046875 -310.19980468745234 -356.60019531245234\n",
      "lp, policy_loss tensor(-9.2749, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2801, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 10605.2001953125 -332.60019531245234 -310.19980468745234\n",
      "lp, policy_loss tensor(-9.2506, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2995, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 5) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 10478.7998046875 -206.19980468745234 -332.60019531245234\n",
      "lp, policy_loss tensor(-9.1857, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1844, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 0) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 10425.599609375 -152.99960937495234 -206.19980468745234\n",
      "lp, policy_loss tensor(-9.2802, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1382, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (1, 1) high_layout\n",
      "Score, reward, prev_reward 10566.7998046875 -294.19980468745234 -152.99960937495234\n",
      "lp, policy_loss tensor(-9.0508, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2592, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 10600.400390625 -327.80039062495234 -294.19980468745234\n",
      "lp, policy_loss tensor(-9.1618, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2924, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 1) between_layouts\n",
      "Score, reward, prev_reward 10597.7998046875 -325.19980468745234 -327.80039062495234\n",
      "lp, policy_loss tensor(-9.0408, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2862, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (1, 11) between_layouts\n",
      "Score, reward, prev_reward 10643.2001953125 -370.60019531245234 -325.19980468745234\n",
      "lp, policy_loss tensor(-9.0156, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (3, 5) between_layouts\n",
      "Score, reward, prev_reward 10551.400390625 -278.80039062495234 -370.60019531245234\n",
      "lp, policy_loss tensor(-9.0228, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2449, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 10) (3, 13) between_layouts\n",
      "Score, reward, prev_reward 10372.7998046875 -100.19980468745234 -278.80039062495234\n",
      "lp, policy_loss tensor(-8.9926, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0877, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (4, 10) high_layout\n",
      "Score, reward, prev_reward 10975.599609375 -702.9996093749523 -100.19980468745234\n",
      "lp, policy_loss tensor(-8.8873, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6082, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (4, 5) between_layouts\n",
      "Score, reward, prev_reward 10952.400390625 -679.8003906249523 -702.9996093749523\n",
      "lp, policy_loss tensor(-9.1418, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6050, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 7) (3, 12) between_layouts\n",
      "Score, reward, prev_reward 10902.0 -629.3999999999523 -679.8003906249523\n",
      "lp, policy_loss tensor(-8.7247, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5346, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (1, 0) high_layout\n",
      "Score, reward, prev_reward 10902.0 -629.3999999999523 -629.3999999999523\n",
      "lp, policy_loss tensor(-9.2816, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5687, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (0, 6) high_layout\n",
      "Score, reward, prev_reward 10932.7998046875 -660.1998046874523 -629.3999999999523\n",
      "lp, policy_loss tensor(-9.2143, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5922, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 10923.0 -650.3999999999523 -660.1998046874523\n",
      "lp, policy_loss tensor(-9.4232, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5966, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (2, 8) high_layout\n",
      "Score, reward, prev_reward 10977.7998046875 -705.1998046874523 -650.3999999999523\n",
      "lp, policy_loss tensor(-9.2983, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6383, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (2, 3) low_layouts\n",
      "Score, reward, prev_reward 10977.7998046875 -705.1998046874523 -705.1998046874523\n",
      "lp, policy_loss tensor(-9.0688, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6226, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (2, 7) between_layouts\n",
      "Score, reward, prev_reward 10993.2001953125 -720.6001953124523 -705.1998046874523\n",
      "lp, policy_loss tensor(-8.9439, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6274, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (0, 10) between_layouts\n",
      "Score, reward, prev_reward 12117.7998046875 -1845.1998046874523 -720.6001953124523\n",
      "lp, policy_loss tensor(-8.9812, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.6132, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 8) (4, 6) low_layouts\n",
      "Score, reward, prev_reward 10034.7998046875 237.80019531254766 -10\n",
      "lp, policy_loss tensor(-9.2090, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2132, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 11) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 10157.599609375 115.00039062504766 237.80019531254766\n",
      "lp, policy_loss tensor(-8.9034, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0997, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 13) (4, 4) low_layouts\n",
      "Score, reward, prev_reward 10157.599609375 115.00039062504766 115.00039062504766\n",
      "lp, policy_loss tensor(-9.0921, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1018, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (1, 10) high_layout\n",
      "Score, reward, prev_reward 9759.7998046875 512.8001953125477 115.00039062504766\n",
      "lp, policy_loss tensor(-9.2220, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4604, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (4, 4) between_layouts\n",
      "Score, reward, prev_reward 9394.599609375 878.0003906250477 512.8001953125477\n",
      "lp, policy_loss tensor(-9.1781, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7845, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 10) (4, 4) between_layouts\n",
      "Score, reward, prev_reward 10344.2001953125 -71.60019531245234 878.0003906250477\n",
      "lp, policy_loss tensor(-9.2364, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0644, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 10344.2001953125 -71.60019531245234 -71.60019531245234\n",
      "lp, policy_loss tensor(-8.9873, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0626, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 11) (0, 13) high_layout\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.0878, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4557e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 7) (1, 4) between_layouts\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.0109, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4265e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (3, 0) between_layouts\n",
      "Score, reward, prev_reward 10381.2001953125 -108.60019531245234 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.0499, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0957, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (2, 1) between_layouts\n",
      "Score, reward, prev_reward 10381.599609375 -108.99960937495234 -108.60019531245234\n",
      "lp, policy_loss tensor(-8.9523, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0950, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (3, 8) high_layout\n",
      "Score, reward, prev_reward 10381.599609375 -108.99960937495234 -108.99960937495234\n",
      "lp, policy_loss tensor(-8.9820, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0953, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 10279.400390625 -6.8003906249523425 -108.99960937495234\n",
      "lp, policy_loss tensor(-9.0755, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0060, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 8) (0, 3) between_layouts\n",
      "Score, reward, prev_reward 10279.400390625 -6.8003906249523425 -6.8003906249523425\n",
      "lp, policy_loss tensor(-9.0271, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0060, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (1, 13) high_layout\n",
      "Score, reward, prev_reward 10257.2001953125 15.399804687547658 -6.8003906249523425\n",
      "lp, policy_loss tensor(-9.0943, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0136, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 5) between_layouts\n",
      "Score, reward, prev_reward 10349.0 -76.39999999995234 15.399804687547658\n",
      "lp, policy_loss tensor(-8.8628, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0659, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (4, 9) high_layout\n",
      "Score, reward, prev_reward 10349.0 -76.39999999995234 -76.39999999995234\n",
      "lp, policy_loss tensor(-9.0001, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0669, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (3, 4) between_layouts\n",
      "Score, reward, prev_reward 10435.7998046875 -163.19980468745234 -76.39999999995234\n",
      "lp, policy_loss tensor(-9.0726, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1441, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 7) (4, 7) high_layout\n",
      "Score, reward, prev_reward 10421.400390625 -148.80039062495234 -163.19980468745234\n",
      "lp, policy_loss tensor(-9.3034, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1348, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (3, 4) high_layout\n",
      "Score, reward, prev_reward 10324.599609375 -51.99960937495234 -148.80039062495234\n",
      "lp, policy_loss tensor(-8.8274, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0447, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (2, 13) low_layouts\n",
      "Score, reward, prev_reward 9786.0 486.60000000004766 -51.99960937495234\n",
      "lp, policy_loss tensor(-8.9325, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4231, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (0, 12) between_layouts\n",
      "Score, reward, prev_reward 9956.599609375 316.00039062504766 486.60000000004766\n",
      "lp, policy_loss tensor(-9.2245, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2838, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (2, 2) high_layout\n",
      "Score, reward, prev_reward 10044.599609375 228.00039062504766 316.00039062504766\n",
      "lp, policy_loss tensor(-9.2185, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2046, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (4, 9) low_layouts\n",
      "Score, reward, prev_reward 10506.400390625 -233.80039062495234 228.00039062504766\n",
      "lp, policy_loss tensor(-9.1809, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2090, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (2, 1) between_layouts\n",
      "Score, reward, prev_reward 10530.2001953125 -257.60019531245234 -233.80039062495234\n",
      "lp, policy_loss tensor(-9.5360, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2391, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (0, 7) between_layouts\n",
      "Score, reward, prev_reward 10480.7998046875 -208.19980468745234 -257.60019531245234\n",
      "lp, policy_loss tensor(-9.2035, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1865, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (4, 6) between_layouts\n",
      "Score, reward, prev_reward 10293.599609375 -20.999609374952342 -208.19980468745234\n",
      "lp, policy_loss tensor(-8.9188, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0182, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (4, 6) low_layouts\n",
      "Score, reward, prev_reward 10092.599609375 180.00039062504766 -20.999609374952342\n",
      "lp, policy_loss tensor(-9.1658, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1606, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (2, 7) high_layout\n",
      "Score, reward, prev_reward 10088.400390625 184.19960937504766 180.00039062504766\n",
      "lp, policy_loss tensor(-9.0917, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1630, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (3, 5) low_layouts\n",
      "Score, reward, prev_reward 10098.2001953125 174.39980468754766 184.19960937504766\n",
      "lp, policy_loss tensor(-9.0142, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1530, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (0, 3) between_layouts\n",
      "Score, reward, prev_reward 10098.2001953125 174.39980468754766 174.39980468754766\n",
      "lp, policy_loss tensor(-9.1579, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1555, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (4, 3) low_layouts\n",
      "Score, reward, prev_reward 10090.2001953125 182.39980468754766 174.39980468754766\n",
      "lp, policy_loss tensor(-9.0467, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1606, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 10309.599609375 -36.99960937495234 182.39980468754766\n",
      "lp, policy_loss tensor(-9.2874, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0335, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (4, 3) low_layouts\n",
      "Score, reward, prev_reward 10537.0 -264.39999999995234 -36.99960937495234\n",
      "lp, policy_loss tensor(-9.2907, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2391, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 10510.0 -237.39999999995234 -264.39999999995234\n",
      "lp, policy_loss tensor(-8.9324, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2064, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (0, 6) between_layouts\n",
      "Score, reward, prev_reward 11092.0 -819.3999999999523 -237.39999999995234\n",
      "lp, policy_loss tensor(-9.1830, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7325, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (3, 13) between_layouts\n",
      "Score, reward, prev_reward 11194.7998046875 -922.1998046874523 -819.3999999999523\n",
      "lp, policy_loss tensor(-9.2647, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8317, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (2, 11) high_layout\n",
      "Score, reward, prev_reward 11184.2001953125 -911.6001953124523 -922.1998046874523\n",
      "lp, policy_loss tensor(-9.2311, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8192, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (0, 7) between_layouts\n",
      "Score, reward, prev_reward 11238.400390625 -965.8003906249523 -911.6001953124523\n",
      "lp, policy_loss tensor(-8.9693, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8433, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (1, 2) between_layouts\n",
      "Score, reward, prev_reward 11272.7998046875 -1000.1998046874523 -965.8003906249523\n",
      "lp, policy_loss tensor(-9.0164, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8779, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 0) (1, 6) between_layouts\n",
      "Score, reward, prev_reward 10249.7998046875 22.800195312547658 -10\n",
      "lp, policy_loss tensor(-8.8519, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0196, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (0, 10) high_layout\n",
      "Score, reward, prev_reward 10511.400390625 -238.80039062495234 22.800195312547658\n",
      "lp, policy_loss tensor(-9.1322, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2123, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (4, 9) low_layouts\n",
      "Score, reward, prev_reward 10491.599609375 -218.99960937495234 -238.80039062495234\n",
      "lp, policy_loss tensor(-9.0963, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1939, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 10503.599609375 -230.99960937495234 -218.99960937495234\n",
      "lp, policy_loss tensor(-9.0628, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2038, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 6) (2, 10) between_layouts\n",
      "Score, reward, prev_reward 10480.7998046875 -208.19980468745234 -230.99960937495234\n",
      "lp, policy_loss tensor(-9.2475, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1874, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (2, 9) low_layouts\n",
      "Score, reward, prev_reward 10437.2001953125 -164.60019531245234 -208.19980468745234\n",
      "lp, policy_loss tensor(-8.9374, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1432, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 11447.400390625 -1174.8003906249523 -164.60019531245234\n",
      "lp, policy_loss tensor(-9.0208, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.0316, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (3, 8) (0, 10) between_layouts\n",
      "Score, reward, prev_reward 10185.599609375 87.00039062504766 -10\n",
      "lp, policy_loss tensor(-9.0129, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0763, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (3, 7) low_layouts\n",
      "Score, reward, prev_reward 10233.599609375 39.00039062504766 87.00039062504766\n",
      "lp, policy_loss tensor(-9.3109, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0353, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (2, 1) low_layouts\n",
      "Score, reward, prev_reward 10190.2001953125 82.39980468754766 39.00039062504766\n",
      "lp, policy_loss tensor(-9.0803, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0728, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 4) (3, 7) high_layout\n",
      "Score, reward, prev_reward 10190.2001953125 82.39980468754766 82.39980468754766\n",
      "lp, policy_loss tensor(-9.0286, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0724, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (2, 0) low_layouts\n",
      "Score, reward, prev_reward 10455.599609375 -182.99960937495234 82.39980468754766\n",
      "lp, policy_loss tensor(-9.0657, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1615, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 11827.0 -1554.3999999999523 -182.99960937495234\n",
      "lp, policy_loss tensor(-9.0844, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.3746, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 4) (4, 7) low_layouts\n",
      "Score, reward, prev_reward 10420.0 -147.39999999995234 -10\n",
      "lp, policy_loss tensor(-8.9471, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1284, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (3, 4) high_layout\n",
      "Score, reward, prev_reward 10420.0 -147.39999999995234 -147.39999999995234\n",
      "lp, policy_loss tensor(-9.2012, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1320, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10529.0 -256.39999999995234 -147.39999999995234\n",
      "lp, policy_loss tensor(-9.1925, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2294, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (2, 8) high_layout\n",
      "Score, reward, prev_reward 10529.0 -256.39999999995234 -256.39999999995234\n",
      "lp, policy_loss tensor(-8.9759, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2240, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (1, 1) between_layouts\n",
      "Score, reward, prev_reward 10536.599609375 -263.99960937495234 -256.39999999995234\n",
      "lp, policy_loss tensor(-8.8968, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2286, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 10562.0 -289.39999999995234 -263.99960937495234\n",
      "lp, policy_loss tensor(-9.1622, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2581, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (0, 11) between_layouts\n",
      "Score, reward, prev_reward 10642.400390625 -369.80039062495234 -289.39999999995234\n",
      "lp, policy_loss tensor(-9.3738, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3374, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (1, 3) between_layouts\n",
      "Score, reward, prev_reward 10611.599609375 -338.99960937495234 -369.80039062495234\n",
      "lp, policy_loss tensor(-9.1331, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3014, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (0, 12) low_layouts\n",
      "Score, reward, prev_reward 10482.2001953125 -209.60019531245234 -338.99960937495234\n",
      "lp, policy_loss tensor(-9.2868, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1895, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (2, 8) between_layouts\n",
      "Score, reward, prev_reward 10507.7998046875 -235.19980468745234 -209.60019531245234\n",
      "lp, policy_loss tensor(-8.8673, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2030, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (2, 1) between_layouts\n",
      "Score, reward, prev_reward 11300.599609375 -1027.9996093749523 -235.19980468745234\n",
      "lp, policy_loss tensor(-9.2610, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.9268, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 0) (4, 3) high_layout\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-8.9416, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4001e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (0, 10) between_layouts\n",
      "Score, reward, prev_reward 10545.0 -272.39999999995234 0.0003906250476575224\n",
      "lp, policy_loss tensor(-8.9308, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2368, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 4) (2, 7) between_layouts\n",
      "Score, reward, prev_reward 10417.7998046875 -145.19980468745234 -272.39999999995234\n",
      "lp, policy_loss tensor(-8.9976, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1272, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (3, 7) high_layout\n",
      "Score, reward, prev_reward 10437.0 -164.39999999995234 -145.19980468745234\n",
      "lp, policy_loss tensor(-8.7434, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1399, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 5) (3, 4) low_layouts\n",
      "Score, reward, prev_reward 10500.2001953125 -227.60019531245234 -164.39999999995234\n",
      "lp, policy_loss tensor(-9.1120, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2019, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (1, 6) between_layouts\n",
      "Score, reward, prev_reward 10609.0 -336.39999999995234 -227.60019531245234\n",
      "lp, policy_loss tensor(-9.3823, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3072, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (2, 12) between_layouts\n",
      "Score, reward, prev_reward 10609.0 -336.39999999995234 -336.39999999995234\n",
      "lp, policy_loss tensor(-9.0150, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2952, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 10) (4, 4) low_layouts\n",
      "Score, reward, prev_reward 10151.599609375 121.00039062504766 -10\n",
      "lp, policy_loss tensor(-9.3362, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1100, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 8) (4, 4) high_layout\n",
      "Score, reward, prev_reward 10417.400390625 -144.80039062495234 121.00039062504766\n",
      "lp, policy_loss tensor(-9.1732, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1293, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (3, 8) low_layouts\n",
      "Score, reward, prev_reward 10849.2001953125 -576.6001953124523 -144.80039062495234\n",
      "lp, policy_loss tensor(-9.1146, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5116, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (0, 2) between_layouts\n",
      "Score, reward, prev_reward 11399.7998046875 -1127.1998046874523 -576.6001953124523\n",
      "lp, policy_loss tensor(-8.8063, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.9663, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 6) (0, 10) high_layout\n",
      "Score, reward, prev_reward 10747.7998046875 -475.19980468745234 -10\n",
      "lp, policy_loss tensor(-8.9092, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4121, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (2, 12) between_layouts\n",
      "Score, reward, prev_reward 9230.400390625 1042.1996093750477 -475.19980468745234\n",
      "lp, policy_loss tensor(-8.7975, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8925, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (1, 1) between_layouts\n",
      "Score, reward, prev_reward 9198.400390625 1074.1996093750477 1042.1996093750477\n",
      "lp, policy_loss tensor(-8.9756, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9386, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 8) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 9198.400390625 1074.1996093750477 1074.1996093750477\n",
      "lp, policy_loss tensor(-8.8393, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9243, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (4, 4) high_layout\n",
      "Score, reward, prev_reward 9245.400390625 1027.1996093750477 1074.1996093750477\n",
      "lp, policy_loss tensor(-9.1361, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9136, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (3, 10) low_layouts\n",
      "Score, reward, prev_reward 9422.7998046875 849.8001953125477 1027.1996093750477\n",
      "lp, policy_loss tensor(-9.0515, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7488, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (4, 2) low_layouts\n",
      "Score, reward, prev_reward 9399.0 873.6000000000477 849.8001953125477\n",
      "lp, policy_loss tensor(-8.9257, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7591, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (0, 3) low_layouts\n",
      "Score, reward, prev_reward 9425.0 847.6000000000477 873.6000000000477\n",
      "lp, policy_loss tensor(-8.8892, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7335, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 2) (2, 13) high_layout\n",
      "Score, reward, prev_reward 9492.2001953125 780.3998046875477 847.6000000000477\n",
      "lp, policy_loss tensor(-9.0240, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6855, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (3, 4) low_layouts\n",
      "Score, reward, prev_reward 9724.400390625 548.1996093750477 780.3998046875477\n",
      "lp, policy_loss tensor(-9.0338, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4821, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (3, 1) high_layout\n",
      "Score, reward, prev_reward 9724.400390625 548.1996093750477 548.1996093750477\n",
      "lp, policy_loss tensor(-9.1686, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4893, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 3) (4, 2) high_layout\n",
      "Score, reward, prev_reward 9722.7998046875 549.8001953125477 548.1996093750477\n",
      "lp, policy_loss tensor(-9.1992, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4924, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (2, 0) high_layout\n",
      "Score, reward, prev_reward 9722.7998046875 549.8001953125477 549.8001953125477\n",
      "lp, policy_loss tensor(-9.0217, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4829, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 9) (3, 4) between_layouts\n",
      "Score, reward, prev_reward 9651.0 621.6000000000477 549.8001953125477\n",
      "lp, policy_loss tensor(-9.3099, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5633, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (2, 5) high_layout\n",
      "Score, reward, prev_reward 9571.0 701.6000000000477 621.6000000000477\n",
      "lp, policy_loss tensor(-9.1976, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6282, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (1, 6) high_layout\n",
      "Score, reward, prev_reward 9597.7998046875 674.8001953125477 701.6000000000477\n",
      "lp, policy_loss tensor(-9.1354, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6001, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (0, 4) between_layouts\n",
      "Score, reward, prev_reward 9924.2001953125 348.39980468754766 674.8001953125477\n",
      "lp, policy_loss tensor(-8.9819, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3046, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (3, 0) high_layout\n",
      "Score, reward, prev_reward 9886.599609375 386.00039062504766 348.39980468754766\n",
      "lp, policy_loss tensor(-9.2387, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3471, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (3, 11) between_layouts\n",
      "Score, reward, prev_reward 9886.599609375 386.00039062504766 386.00039062504766\n",
      "lp, policy_loss tensor(-9.1583, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3441, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 9924.7998046875 347.80019531254766 386.00039062504766\n",
      "lp, policy_loss tensor(-8.9297, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3023, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 2) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 9924.7998046875 347.80019531254766 347.80019531254766\n",
      "lp, policy_loss tensor(-9.0702, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3071, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 4) (0, 1) between_layouts\n",
      "Score, reward, prev_reward 9893.599609375 379.00039062504766 347.80019531254766\n",
      "lp, policy_loss tensor(-9.1006, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3358, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (1, 4) between_layouts\n",
      "Score, reward, prev_reward 9896.0 376.60000000004766 379.00039062504766\n",
      "lp, policy_loss tensor(-9.2112, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3377, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (3, 7) high_layout\n",
      "Score, reward, prev_reward 9889.0 383.60000000004766 376.60000000004766\n",
      "lp, policy_loss tensor(-9.0185, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3368, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (2, 10) low_layouts\n",
      "Score, reward, prev_reward 9889.0 383.60000000004766 383.60000000004766\n",
      "lp, policy_loss tensor(-9.2453, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3452, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 2) low_layouts\n",
      "Score, reward, prev_reward 9889.0 383.60000000004766 383.60000000004766\n",
      "lp, policy_loss tensor(-8.9923, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3358, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 10) (2, 2) low_layouts\n",
      "Score, reward, prev_reward 9873.0 399.60000000004766 383.60000000004766\n",
      "lp, policy_loss tensor(-8.9583, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3485, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 6) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 9880.2001953125 392.39980468754766 399.60000000004766\n",
      "lp, policy_loss tensor(-8.9409, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3415, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (0, 9) high_layout\n",
      "Score, reward, prev_reward 9501.400390625 771.1996093750477 392.39980468754766\n",
      "lp, policy_loss tensor(-9.0965, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6829, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 10) (3, 6) between_layouts\n",
      "Score, reward, prev_reward 9447.7998046875 824.8001953125477 771.1996093750477\n",
      "lp, policy_loss tensor(-9.0544, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7270, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (2, 9) high_layout\n",
      "Score, reward, prev_reward 9438.7998046875 833.8001953125477 824.8001953125477\n",
      "lp, policy_loss tensor(-9.0536, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7349, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 5) high_layout\n",
      "Score, reward, prev_reward 9402.7998046875 869.8001953125477 833.8001953125477\n",
      "lp, policy_loss tensor(-8.9811, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7604, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (1, 12) low_layouts\n",
      "Score, reward, prev_reward 9352.0 920.6000000000477 869.8001953125477\n",
      "lp, policy_loss tensor(-9.1518, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8202, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 10) (2, 4) low_layouts\n",
      "Score, reward, prev_reward 9354.599609375 918.0003906250477 920.6000000000477\n",
      "lp, policy_loss tensor(-9.0245, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8065, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (4, 5) between_layouts\n",
      "Score, reward, prev_reward 9354.599609375 918.0003906250477 918.0003906250477\n",
      "lp, policy_loss tensor(-8.9316, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7982, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (1, 9) between_layouts\n",
      "Score, reward, prev_reward 12690.7998046875 -2418.1998046874523 918.0003906250477\n",
      "lp, policy_loss tensor(-9.1322, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-2.1497, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 13) (2, 4) low_layouts\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.0479, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.4405e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 0) between_layouts\n",
      "Score, reward, prev_reward 11351.400390625 -1078.8003906249523 0.0003906250476575224\n",
      "lp, policy_loss tensor(-8.9675, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.9417, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (4, 4) (1, 13) between_layouts\n",
      "Score, reward, prev_reward 10237.2001953125 35.39980468754766 -10\n",
      "lp, policy_loss tensor(-8.9679, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0309, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (3, 8) low_layouts\n",
      "Score, reward, prev_reward 10570.0 -297.39999999995234 35.39980468754766\n",
      "lp, policy_loss tensor(-9.1183, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2640, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (3, 4) high_layout\n",
      "Score, reward, prev_reward 10336.2001953125 -63.60019531245234 -297.39999999995234\n",
      "lp, policy_loss tensor(-8.8519, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0548, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 6) (2, 6) high_layout\n",
      "Score, reward, prev_reward 10328.2001953125 -55.60019531245234 -63.60019531245234\n",
      "lp, policy_loss tensor(-9.1612, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0496, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 10350.0 -77.39999999995234 -55.60019531245234\n",
      "lp, policy_loss tensor(-9.2497, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0697, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (1, 9) low_layouts\n",
      "Score, reward, prev_reward 10284.0 -11.399999999952342 -77.39999999995234\n",
      "lp, policy_loss tensor(-8.9642, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0099, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 6) (3, 6) between_layouts\n",
      "Score, reward, prev_reward 10291.2001953125 -18.600195312452342 -11.399999999952342\n",
      "lp, policy_loss tensor(-9.1008, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0165, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (4, 1) low_layouts\n",
      "Score, reward, prev_reward 10298.400390625 -25.800390624952342 -18.600195312452342\n",
      "lp, policy_loss tensor(-8.9140, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0224, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (1, 3) high_layout\n",
      "Score, reward, prev_reward 10337.2001953125 -64.60019531245234 -25.800390624952342\n",
      "lp, policy_loss tensor(-9.1277, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0574, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 7) (2, 11) between_layouts\n",
      "Score, reward, prev_reward 10446.400390625 -173.80039062495234 -64.60019531245234\n",
      "lp, policy_loss tensor(-9.0761, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1536, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (4, 7) high_layout\n",
      "Score, reward, prev_reward 10393.599609375 -120.99960937495234 -173.80039062495234\n",
      "lp, policy_loss tensor(-8.9456, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1054, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (1, 4) low_layouts\n",
      "Score, reward, prev_reward 9014.400390625 1258.1996093750477 -120.99960937495234\n",
      "lp, policy_loss tensor(-9.1172, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.1167, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (2, 10) between_layouts\n",
      "Score, reward, prev_reward 9030.599609375 1242.0003906250477 1258.1996093750477\n",
      "lp, policy_loss tensor(-9.2928, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.1235, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (3, 1) high_layout\n",
      "Score, reward, prev_reward 9499.599609375 773.0003906250477 1242.0003906250477\n",
      "lp, policy_loss tensor(-9.0888, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6839, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 9704.2001953125 568.3998046875477 773.0003906250477\n",
      "lp, policy_loss tensor(-8.9266, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4939, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (3, 0) high_layout\n",
      "Score, reward, prev_reward 9709.400390625 563.1996093750477 568.3998046875477\n",
      "lp, policy_loss tensor(-9.0356, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4954, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (1, 1) low_layouts\n",
      "Score, reward, prev_reward 9674.7998046875 597.8001953125477 563.1996093750477\n",
      "lp, policy_loss tensor(-9.1033, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5298, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 9241.0 1031.6000000000477 597.8001953125477\n",
      "lp, policy_loss tensor(-9.0348, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9073, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (3, 7) high_layout\n",
      "Score, reward, prev_reward 9243.400390625 1029.1996093750477 1031.6000000000477\n",
      "lp, policy_loss tensor(-9.0344, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9052, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 9) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 9281.2001953125 991.3998046875477 1029.1996093750477\n",
      "lp, policy_loss tensor(-9.0523, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.8736, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 10) (1, 5) between_layouts\n",
      "Score, reward, prev_reward 9215.2001953125 1057.3998046875477 991.3998046875477\n",
      "lp, policy_loss tensor(-9.0217, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9286, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (2, 12) high_layout\n",
      "Score, reward, prev_reward 9225.2001953125 1047.3998046875477 1057.3998046875477\n",
      "lp, policy_loss tensor(-9.2551, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.9437, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 9603.0 669.6000000000477 1047.3998046875477\n",
      "lp, policy_loss tensor(-8.9299, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5821, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (2, 8) between_layouts\n",
      "Score, reward, prev_reward 9616.7998046875 655.8001953125477 669.6000000000477\n",
      "lp, policy_loss tensor(-8.8928, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5677, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (1, 9) low_layouts\n",
      "Score, reward, prev_reward 11161.599609375 -888.9996093749523 655.8001953125477\n",
      "lp, policy_loss tensor(-9.0417, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7825, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (2, 1) high_layout\n",
      "Score, reward, prev_reward 11208.400390625 -935.8003906249523 -888.9996093749523\n",
      "lp, policy_loss tensor(-9.2735, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8448, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (4, 3) high_layout\n",
      "Score, reward, prev_reward 11208.400390625 -935.8003906249523 -935.8003906249523\n",
      "lp, policy_loss tensor(-8.8908, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8099, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 3) (4, 7) low_layouts\n",
      "Score, reward, prev_reward 11214.2001953125 -941.6001953124523 -935.8003906249523\n",
      "lp, policy_loss tensor(-9.0436, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8289, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (3, 2) between_layouts\n",
      "Score, reward, prev_reward 11214.2001953125 -941.6001953124523 -941.6001953124523\n",
      "lp, policy_loss tensor(-9.1432, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8381, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (1, 5) high_layout\n",
      "Score, reward, prev_reward 11170.2001953125 -897.6001953124523 -941.6001953124523\n",
      "lp, policy_loss tensor(-8.8275, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7713, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (1, 7) high_layout\n",
      "Score, reward, prev_reward 11170.2001953125 -897.6001953124523 -897.6001953124523\n",
      "lp, policy_loss tensor(-9.2143, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8051, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 13946.7998046875 -3674.1998046874523 -897.6001953124523\n",
      "lp, policy_loss tensor(-8.9582, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-3.2041, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 5) (3, 10) between_layouts\n",
      "Score, reward, prev_reward 10314.2001953125 -41.60019531245234 -10\n",
      "lp, policy_loss tensor(-8.9932, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0364, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 8) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 10332.599609375 -59.99960937495234 -41.60019531245234\n",
      "lp, policy_loss tensor(-9.1536, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0535, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 9) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 10172.2001953125 100.39980468754766 -59.99960937495234\n",
      "lp, policy_loss tensor(-9.2089, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0900, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (1, 0) low_layouts\n",
      "Score, reward, prev_reward 10140.400390625 132.19960937504766 100.39980468754766\n",
      "lp, policy_loss tensor(-8.9856, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1156, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (2, 2) high_layout\n",
      "Score, reward, prev_reward 10160.400390625 112.19960937504766 132.19960937504766\n",
      "lp, policy_loss tensor(-9.0693, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0991, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 2) (4, 10) high_layout\n",
      "Score, reward, prev_reward 10160.400390625 112.19960937504766 112.19960937504766\n",
      "lp, policy_loss tensor(-9.2311, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1008, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (1, 4) between_layouts\n",
      "Score, reward, prev_reward 10140.7998046875 131.80019531254766 112.19960937504766\n",
      "lp, policy_loss tensor(-9.1198, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1170, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (4, 4) between_layouts\n",
      "Score, reward, prev_reward 10253.599609375 19.000390625047658 131.80019531254766\n",
      "lp, policy_loss tensor(-9.0668, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0168, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 10566.0 -293.39999999995234 19.000390625047658\n",
      "lp, policy_loss tensor(-9.3224, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2663, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 3) (4, 8) high_layout\n",
      "Score, reward, prev_reward 10640.7998046875 -368.19980468745234 -293.39999999995234\n",
      "lp, policy_loss tensor(-9.2253, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3307, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 10) (2, 6) low_layouts\n",
      "Score, reward, prev_reward 10650.400390625 -377.80039062495234 -368.19980468745234\n",
      "lp, policy_loss tensor(-9.4572, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3478, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 6) (3, 4) high_layout\n",
      "Score, reward, prev_reward 10640.7998046875 -368.19980468745234 -377.80039062495234\n",
      "lp, policy_loss tensor(-9.1320, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3273, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (2, 12) low_layouts\n",
      "Score, reward, prev_reward 10528.599609375 -255.99960937495234 -368.19980468745234\n",
      "lp, policy_loss tensor(-9.2570, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2307, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (1, 10) between_layouts\n",
      "Score, reward, prev_reward 10454.599609375 -181.99960937495234 -255.99960937495234\n",
      "lp, policy_loss tensor(-9.0501, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1603, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (0, 0) between_layouts\n",
      "Score, reward, prev_reward 10468.7998046875 -196.19980468745234 -181.99960937495234\n",
      "lp, policy_loss tensor(-9.3345, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1783, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 10493.599609375 -220.99960937495234 -196.19980468745234\n",
      "lp, policy_loss tensor(-9.2185, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1983, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 4) (4, 9) low_layouts\n",
      "Score, reward, prev_reward 10493.599609375 -220.99960937495234 -220.99960937495234\n",
      "lp, policy_loss tensor(-9.2904, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1999, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (0, 6) between_layouts\n",
      "Score, reward, prev_reward 11213.400390625 -940.8003906249523 -220.99960937495234\n",
      "lp, policy_loss tensor(-8.9754, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8220, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 11) (3, 5) high_layout\n",
      "Score, reward, prev_reward 11213.400390625 -940.8003906249523 -940.8003906249523\n",
      "lp, policy_loss tensor(-9.0916, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8326, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (1, 13) high_layout\n",
      "Score, reward, prev_reward 11233.0 -960.3999999999523 -940.8003906249523\n",
      "lp, policy_loss tensor(-8.9950, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8410, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 7) (3, 9) low_layouts\n",
      "Score, reward, prev_reward 11135.0 -862.3999999999523 -960.3999999999523\n",
      "lp, policy_loss tensor(-9.1255, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.7661, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 12) (3, 7) low_layouts\n",
      "Score, reward, prev_reward 12926.7998046875 -2654.1998046874523 -862.3999999999523\n",
      "lp, policy_loss tensor(-8.9828, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-2.3209, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 6) (3, 13) low_layouts\n",
      "Score, reward, prev_reward 10414.7998046875 -142.19980468745234 -10\n",
      "lp, policy_loss tensor(-9.0335, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1250, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 8) (1, 4) between_layouts\n",
      "Score, reward, prev_reward 10444.7998046875 -172.19980468745234 -142.19980468745234\n",
      "lp, policy_loss tensor(-9.0450, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1516, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (4, 2) between_layouts\n",
      "Score, reward, prev_reward 10526.0 -253.39999999995234 -172.19980468745234\n",
      "lp, policy_loss tensor(-8.9720, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2213, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (4, 4) low_layouts\n",
      "Score, reward, prev_reward 10851.7998046875 -579.1998046874523 -253.39999999995234\n",
      "lp, policy_loss tensor(-9.1942, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5184, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (4, 8) high_layout\n",
      "Score, reward, prev_reward 10851.7998046875 -579.1998046874523 -579.1998046874523\n",
      "lp, policy_loss tensor(-8.9055, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5021, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (2, 11) low_layouts\n",
      "Score, reward, prev_reward 10834.7998046875 -562.1998046874523 -579.1998046874523\n",
      "lp, policy_loss tensor(-9.0355, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4945, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 10830.0 -557.3999999999523 -562.1998046874523\n",
      "lp, policy_loss tensor(-8.7383, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4741, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (1, 3) high_layout\n",
      "Score, reward, prev_reward 10910.0 -637.3999999999523 -557.3999999999523\n",
      "lp, policy_loss tensor(-9.1082, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.5651, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (4, 8) between_layouts\n",
      "Score, reward, prev_reward 11195.7998046875 -923.1998046874523 -637.3999999999523\n",
      "lp, policy_loss tensor(-9.0113, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.8098, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 3) (4, 4) high_layout\n",
      "Score, reward, prev_reward 11029.2001953125 -756.6001953124523 -923.1998046874523\n",
      "lp, policy_loss tensor(-9.2691, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.6827, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 10045.599609375 227.00039062504766 -756.6001953124523\n",
      "lp, policy_loss tensor(-8.9931, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1987, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (4, 9) low_layouts\n",
      "Score, reward, prev_reward 10473.599609375 -200.99960937495234 227.00039062504766\n",
      "lp, policy_loss tensor(-9.1996, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1800, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (4, 1) between_layouts\n",
      "Score, reward, prev_reward 10473.599609375 -200.99960937495234 -200.99960937495234\n",
      "lp, policy_loss tensor(-8.9067, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1743, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 12) (1, 3) high_layout\n",
      "Score, reward, prev_reward 10221.400390625 51.19960937504766 -200.99960937495234\n",
      "lp, policy_loss tensor(-9.3367, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0465, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 3) (4, 6) low_layouts\n",
      "Score, reward, prev_reward 10214.2001953125 58.39980468754766 51.19960937504766\n",
      "lp, policy_loss tensor(-9.0661, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0515, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 9) (3, 2) high_layout\n",
      "Score, reward, prev_reward 9942.599609375 330.00039062504766 -10\n",
      "lp, policy_loss tensor(-8.9416, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2872, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 12) (0, 1) between_layouts\n",
      "Score, reward, prev_reward 9952.599609375 320.00039062504766 330.00039062504766\n",
      "lp, policy_loss tensor(-9.0213, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2810, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 11) (3, 4) between_layouts\n",
      "Score, reward, prev_reward 9959.7998046875 312.80019531254766 320.00039062504766\n",
      "lp, policy_loss tensor(-8.8553, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2696, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (2, 4) between_layouts\n",
      "Score, reward, prev_reward 9512.0 760.6000000000477 312.80019531254766\n",
      "lp, policy_loss tensor(-8.9702, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6642, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 9551.0 721.6000000000477 760.6000000000477\n",
      "lp, policy_loss tensor(-9.3238, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6549, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 10) (4, 5) low_layouts\n",
      "Score, reward, prev_reward 9551.0 721.6000000000477 721.6000000000477\n",
      "lp, policy_loss tensor(-9.0705, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6372, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 10) (2, 10) low_layouts\n",
      "Score, reward, prev_reward 9458.0 814.6000000000477 721.6000000000477\n",
      "lp, policy_loss tensor(-8.9236, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7076, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (4, 0) low_layouts\n",
      "Score, reward, prev_reward 9625.7998046875 646.8001953125477 814.6000000000477\n",
      "lp, policy_loss tensor(-9.1878, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5785, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 5) (3, 8) high_layout\n",
      "Score, reward, prev_reward 9625.7998046875 646.8001953125477 646.8001953125477\n",
      "lp, policy_loss tensor(-9.3092, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5861, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (4, 3) high_layout\n",
      "Score, reward, prev_reward 9625.7998046875 646.8001953125477 646.8001953125477\n",
      "lp, policy_loss tensor(-9.0638, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5707, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 4) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 9625.7998046875 646.8001953125477 646.8001953125477\n",
      "lp, policy_loss tensor(-9.3266, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5872, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (3, 2) (3, 12) high_layout\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.2162, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.5046e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 2) (1, 5) low_layouts\n",
      "Score, reward, prev_reward 10252.599609375 20.000390625047658 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.1765, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.0179, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 3) (3, 8) between_layouts\n",
      "Score, reward, prev_reward 10309.2001953125 -36.60019531245234 20.000390625047658\n",
      "lp, policy_loss tensor(-9.1229, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0325, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 2) (1, 11) between_layouts\n",
      "Score, reward, prev_reward 10367.0 -94.39999999995234 -36.60019531245234\n",
      "lp, policy_loss tensor(-8.7642, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0805, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 11) (1, 6) between_layouts\n",
      "Score, reward, prev_reward 10357.7998046875 -85.19980468745234 -94.39999999995234\n",
      "lp, policy_loss tensor(-9.1878, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0762, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (3, 11) high_layout\n",
      "Score, reward, prev_reward 10357.7998046875 -85.19980468745234 -85.19980468745234\n",
      "lp, policy_loss tensor(-8.9789, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0745, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 8) (2, 13) low_layouts\n",
      "Score, reward, prev_reward 10149.7998046875 122.80019531254766 -85.19980468745234\n",
      "lp, policy_loss tensor(-9.0839, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1086, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (4, 1) between_layouts\n",
      "Score, reward, prev_reward 10149.7998046875 122.80019531254766 122.80019531254766\n",
      "lp, policy_loss tensor(-9.1551, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.1094, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (3, 12) (4, 0) high_layout\n",
      "Score, reward, prev_reward 10272.599609375 0.0003906250476575224 -10\n",
      "lp, policy_loss tensor(-9.2590, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(3.5208e-07, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (1, 7) between_layouts\n",
      "Score, reward, prev_reward 10316.599609375 -43.99960937495234 0.0003906250476575224\n",
      "lp, policy_loss tensor(-9.0306, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0387, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 8) (2, 12) low_layouts\n",
      "Score, reward, prev_reward 8644.2001953125 1628.3998046875477 -43.99960937495234\n",
      "lp, policy_loss tensor(-9.0394, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.4329, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 4) (2, 2) high_layout\n",
      "Score, reward, prev_reward 8652.2001953125 1620.3998046875477 1628.3998046875477\n",
      "lp, policy_loss tensor(-9.2369, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.4570, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 6) (0, 12) between_layouts\n",
      "Score, reward, prev_reward 8286.2001953125 1986.3998046875477 1620.3998046875477\n",
      "lp, policy_loss tensor(-9.3362, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(1.8053, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (1, 13) low_layouts\n",
      "Score, reward, prev_reward 9492.7998046875 779.8001953125477 1986.3998046875477\n",
      "lp, policy_loss tensor(-9.3450, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7094, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (3, 12) low_layouts\n",
      "Score, reward, prev_reward 10416.400390625 -143.80039062495234 779.8001953125477\n",
      "lp, policy_loss tensor(-9.1485, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1281, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (2, 4) between_layouts\n",
      "Score, reward, prev_reward 10416.400390625 -143.80039062495234 -143.80039062495234\n",
      "lp, policy_loss tensor(-9.1256, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1277, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 9) (1, 13) between_layouts\n",
      "Score, reward, prev_reward 9474.7998046875 797.8001953125477 -143.80039062495234\n",
      "lp, policy_loss tensor(-9.0547, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7032, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 13) (3, 4) between_layouts\n",
      "Score, reward, prev_reward 9474.7998046875 797.8001953125477 797.8001953125477\n",
      "lp, policy_loss tensor(-9.0366, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7018, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 4) (2, 8) low_layouts\n",
      "Score, reward, prev_reward 9489.599609375 783.0003906250477 797.8001953125477\n",
      "lp, policy_loss tensor(-9.0338, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6886, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 12) (3, 5) low_layouts\n",
      "Score, reward, prev_reward 9489.599609375 783.0003906250477 783.0003906250477\n",
      "lp, policy_loss tensor(-9.0075, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6866, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (1, 10) low_layouts\n",
      "Score, reward, prev_reward 9420.599609375 852.0003906250477 783.0003906250477\n",
      "lp, policy_loss tensor(-8.9691, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7439, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 0) (4, 10) high_layout\n",
      "Score, reward, prev_reward 9420.599609375 852.0003906250477 852.0003906250477\n",
      "lp, policy_loss tensor(-9.1061, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.7553, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (0, 5) between_layouts\n",
      "Score, reward, prev_reward 9538.599609375 734.0003906250477 852.0003906250477\n",
      "lp, policy_loss tensor(-9.1630, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6547, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 8) (2, 5) between_layouts\n",
      "Score, reward, prev_reward 9538.599609375 734.0003906250477 734.0003906250477\n",
      "lp, policy_loss tensor(-8.9701, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6409, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (2, 9) high_layout\n",
      "Score, reward, prev_reward 9538.599609375 734.0003906250477 734.0003906250477\n",
      "lp, policy_loss tensor(-9.0659, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6478, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 13) (4, 4) high_layout\n",
      "Score, reward, prev_reward 9538.599609375 734.0003906250477 734.0003906250477\n",
      "lp, policy_loss tensor(-9.0993, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6502, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 0) (2, 7) between_layouts\n",
      "Score, reward, prev_reward 9548.2001953125 724.3998046875477 734.0003906250477\n",
      "lp, policy_loss tensor(-9.2639, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6533, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 8) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 9603.0 669.6000000000477 724.3998046875477\n",
      "lp, policy_loss tensor(-9.1981, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5996, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 6) (2, 13) low_layouts\n",
      "Score, reward, prev_reward 9590.7998046875 681.8001953125477 669.6000000000477\n",
      "lp, policy_loss tensor(-9.1728, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6088, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (0, 4) between_layouts\n",
      "Score, reward, prev_reward 9591.7998046875 680.8001953125477 681.8001953125477\n",
      "lp, policy_loss tensor(-9.1808, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6084, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (4, 3) between_layouts\n",
      "Score, reward, prev_reward 9558.2001953125 714.3998046875477 680.8001953125477\n",
      "lp, policy_loss tensor(-9.1675, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6375, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 3) (0, 13) low_layouts\n",
      "Score, reward, prev_reward 9558.2001953125 714.3998046875477 714.3998046875477\n",
      "lp, policy_loss tensor(-9.0717, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6309, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 3) (3, 1) high_layout\n",
      "Score, reward, prev_reward 9558.2001953125 714.3998046875477 714.3998046875477\n",
      "lp, policy_loss tensor(-9.1664, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6375, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (1, 5) low_layouts\n",
      "Score, reward, prev_reward 9565.2001953125 707.3998046875477 714.3998046875477\n",
      "lp, policy_loss tensor(-9.1379, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6293, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (1, 2) high_layout\n",
      "Score, reward, prev_reward 9520.599609375 752.0003906250477 707.3998046875477\n",
      "lp, policy_loss tensor(-8.9826, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.6576, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 1) (2, 11) between_layouts\n",
      "Score, reward, prev_reward 9720.400390625 552.1996093750477 752.0003906250477\n",
      "lp, policy_loss tensor(-8.8993, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4784, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (1, 3) between_layouts\n",
      "Score, reward, prev_reward 9730.0 542.6000000000477 552.1996093750477\n",
      "lp, policy_loss tensor(-9.1374, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4826, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 8) (3, 0) between_layouts\n",
      "Score, reward, prev_reward 9730.0 542.6000000000477 542.6000000000477\n",
      "lp, policy_loss tensor(-9.1393, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4827, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (1, 4) low_layouts\n",
      "Score, reward, prev_reward 9717.599609375 555.0003906250477 542.6000000000477\n",
      "lp, policy_loss tensor(-8.8887, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4802, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 5) (3, 9) high_layout\n",
      "Score, reward, prev_reward 9715.7998046875 556.8001953125477 555.0003906250477\n",
      "lp, policy_loss tensor(-8.9657, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.4860, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 13) (1, 10) low_layouts\n",
      "Score, reward, prev_reward 13033.599609375 -2760.9996093749523 556.8001953125477\n",
      "lp, policy_loss tensor(-9.0495, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-2.4323, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (2, 8) (4, 5) between_layouts\n",
      "Score, reward, prev_reward 10309.400390625 -36.80039062495234 -10\n",
      "lp, policy_loss tensor(-8.9791, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.0322, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 9) (3, 4) between_layouts\n",
      "Score, reward, prev_reward 10428.400390625 -155.80039062495234 -36.80039062495234\n",
      "lp, policy_loss tensor(-9.0805, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1377, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 13) (3, 9) between_layouts\n",
      "Score, reward, prev_reward 10759.599609375 -486.99960937495234 -155.80039062495234\n",
      "lp, policy_loss tensor(-8.9468, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.4241, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 8) (0, 8) between_layouts\n",
      "Score, reward, prev_reward 11457.2001953125 -1184.6001953124523 -486.99960937495234\n",
      "lp, policy_loss tensor(-8.9046, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-1.0268, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (0, 1) (1, 12) between_layouts\n",
      "Score, reward, prev_reward 9689.2001953125 583.3998046875477 -10\n",
      "lp, policy_loss tensor(-9.2717, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.5266, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 11) (2, 4) low_layouts\n",
      "Score, reward, prev_reward 9833.599609375 439.00039062504766 583.3998046875477\n",
      "lp, policy_loss tensor(-8.8307, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3774, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (3, 1) (4, 10) between_layouts\n",
      "Score, reward, prev_reward 10464.599609375 -191.99960937495234 439.00039062504766\n",
      "lp, policy_loss tensor(-9.1751, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1715, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 8) (3, 6) high_layout\n",
      "Score, reward, prev_reward 10470.2001953125 -197.60019531245234 -191.99960937495234\n",
      "lp, policy_loss tensor(-9.2799, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1785, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 7) (3, 12) between_layouts\n",
      "Score, reward, prev_reward 9856.0 416.60000000004766 -197.60019531245234\n",
      "lp, policy_loss tensor(-8.6821, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.3521, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (3, 9) high_layout\n",
      "Score, reward, prev_reward 9934.7998046875 337.80019531254766 416.60000000004766\n",
      "lp, policy_loss tensor(-8.9799, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2953, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 1) (3, 7) between_layouts\n",
      "Score, reward, prev_reward 9950.0 322.60000000004766 337.80019531254766\n",
      "lp, policy_loss tensor(-8.7508, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(0.2748, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 9) (0, 4) between_layouts\n",
      "Score, reward, prev_reward 10436.400390625 -163.80039062495234 322.60000000004766\n",
      "lp, policy_loss tensor(-9.1363, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1457, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 0) (4, 8) low_layouts\n",
      "Score, reward, prev_reward 10436.400390625 -163.80039062495234 -163.80039062495234\n",
      "lp, policy_loss tensor(-8.8468, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1411, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 7) (0, 1) between_layouts\n",
      "Score, reward, prev_reward 10703.400390625 -430.80039062495234 -163.80039062495234\n",
      "lp, policy_loss tensor(-9.0139, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3780, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 1) (3, 2) low_layouts\n",
      "Score, reward, prev_reward 10703.400390625 -430.80039062495234 -430.80039062495234\n",
      "lp, policy_loss tensor(-9.0982, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3815, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (2, 0) (3, 9) high_layout\n",
      "Score, reward, prev_reward 10698.2001953125 -425.60019531245234 -430.80039062495234\n",
      "lp, policy_loss tensor(-9.3071, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3856, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 5) (4, 8) low_layouts\n",
      "Score, reward, prev_reward 10695.7998046875 -423.19980468745234 -425.60019531245234\n",
      "lp, policy_loss tensor(-9.0984, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.3748, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (0, 12) low_layouts\n",
      "Score, reward, prev_reward 10554.7998046875 -282.19980468745234 -423.19980468745234\n",
      "lp, policy_loss tensor(-9.3061, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2556, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 7) (4, 1) high_layout\n",
      "Score, reward, prev_reward 10550.599609375 -277.99960937495234 -282.19980468745234\n",
      "lp, policy_loss tensor(-9.3343, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2526, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 1) (3, 0) high_layout\n",
      "Score, reward, prev_reward 10567.400390625 -294.80039062495234 -277.99960937495234\n",
      "lp, policy_loss tensor(-9.1843, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2636, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (4, 5) (4, 7) between_layouts\n",
      "Score, reward, prev_reward 10567.400390625 -294.80039062495234 -294.80039062495234\n",
      "lp, policy_loss tensor(-8.9509, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.2569, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "fail\n",
      "\n",
      "Positions (1, 9) (1, 2) between_layouts\n",
      "Score, reward, prev_reward 10407.400390625 -134.80039062495234 -10\n",
      "lp, policy_loss tensor(-9.2977, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1220, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (0, 4) (0, 12) high_layout\n",
      "Score, reward, prev_reward 10393.7998046875 -121.19980468745234 -134.80039062495234\n",
      "lp, policy_loss tensor(-9.0267, device='cuda:0', grad_fn=<SqueezeBackward1>) tensor(-0.1065, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Positions (1, 5) (3, 1) high_layout\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\notebooks\\actor-critic-td0.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m new_state\u001b[39m.\u001b[39mswap_buttons(pos1, pos2, \u001b[39mtype\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPositions\u001b[39m\u001b[39m\"\u001b[39m, pos1, pos2, \u001b[39mtype\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m new_score \u001b[39m=\u001b[39m estimate_layout(new_state, dataset[:\u001b[39m10\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m new_score\u001b[39m.\u001b[39mitem() \u001b[39m<\u001b[39m best_score:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     best_score \u001b[39m=\u001b[39m new_score\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\notebooks\\actor-critic-td0.ipynb Cell 35\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m layout\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m loop:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     layout\u001b[39m.\u001b[39;49mtype_encoded_text(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m score \u001b[39m=\u001b[39m layout\u001b[39m.\u001b[39mtotal_score\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# scores.append(layout.get_average_score())\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\notebooks\\actor-critic-td0.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtype_encoded_text\u001b[39m(\u001b[39mself\u001b[39m, encoded_text: \u001b[39mlist\u001b[39m[\u001b[39mint\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     \u001b[39mfor\u001b[39;00m button \u001b[39min\u001b[39;00m encoded_text:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_button(button)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_score\n",
      "\u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\notebooks\\actor-critic-td0.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_button\u001b[39m(\u001b[39mself\u001b[39m, button: \u001b[39mint\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39mif\u001b[39;00m button \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_layout_dict \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_layout_dict[button]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m         (finger_id, finger_position), score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmove_one_finger(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlow_layout_dict[button]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m         )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfingers[finger_id]\u001b[39m.\u001b[39mmove(finger_position)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m score\n",
      "\u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\notebooks\\actor-critic-td0.ipynb Cell 35\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m position \u001b[39min\u001b[39;00m positions:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     scores \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         finger\u001b[39m.\u001b[39mget_score(position) \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m busy_finger_id \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39minf\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i, finger \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfingers)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     ]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     candidate_finger_id \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39;49margmin(scores))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     candidate_score \u001b[39m=\u001b[39m scores[candidate_finger_id]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kadav/VSCodeProjects/Project/RL_for_keyboard_layout/notebooks/actor-critic-td0.ipynb#Y121sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mif\u001b[39;00m candidate_score \u001b[39m<\u001b[39m best_score:\n",
      "File \u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1325\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[39mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1325\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmin\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\kadav\\VSCodeProjects\\Project\\RL_for_keyboard_layout\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# track scores\n",
    "scores = []\n",
    "\n",
    "# track recent scores\n",
    "recent_scores = deque(maxlen=100)\n",
    "\n",
    "# run episodes\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # init variables\n",
    "    state = state = KeyboardLayout(\n",
    "        QWERTY_ENCODED_LOW, QWERTY_ENCODED_HIGH, Logger(verbose=False)\n",
    "    )\n",
    "    done = False\n",
    "    score = 0\n",
    "    prev_reward = -10\n",
    "    I = 1\n",
    "    prev_states = [(state.low_layout, state.high_layout)]\n",
    "\n",
    "    # run episode, update online\n",
    "    for step in range(MAX_STEPS):\n",
    "        # get action and log probability\n",
    "        action, lp = select_action(policy_network, state)\n",
    "        # print(\"lp\", lp)\n",
    "        # step with action\n",
    "        btn1, btn2 = ID_TO_PAIR[action]\n",
    "        cord1, cord2 = convert_int_to_cord(btn1), convert_int_to_cord(btn2)\n",
    "\n",
    "        type = \"low_layouts\"\n",
    "\n",
    "        if cord2[0] == 1:\n",
    "            type = \"between_layouts\"\n",
    "\n",
    "        if cord1[0] == 1:\n",
    "            type = \"high_layout\"\n",
    "\n",
    "        pos1 = (cord1[1], cord1[2])\n",
    "        pos2 = (cord2[1], cord2[2])\n",
    "        new_state = deepcopy(state)\n",
    "        new_state.swap_buttons(pos1, pos2, type)\n",
    "        print(\"Positions\", pos1, pos2, type)\n",
    "\n",
    "        new_score = estimate_layout(new_state, dataset[:10])\n",
    "\n",
    "        if new_score.item() < best_score:\n",
    "            best_score = new_score.item()\n",
    "            best_keyboard = deepcopy(new_state)\n",
    "        # if new_score.item() < qwerty_score_total - 1:\n",
    "        reward = qwerty_score_total - new_score.item()\n",
    "        # else:\n",
    "        #     reward = 0\n",
    "        if (new_state.low_layout, new_state.high_layout) in prev_states or reward < -1000:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        # if done:\n",
    "        #     reward = -100000\n",
    "        # if prev_reward > 0 and done:\n",
    "        #     reward = -1000\n",
    "        print(\"Score, reward, prev_reward\", new_score.item(), reward, prev_reward)\n",
    "        prev_reward = reward\n",
    "        cur_score = new_score.item()\n",
    "\n",
    "        # update episode score\n",
    "        score += reward\n",
    "\n",
    "        # get state value of current state\n",
    "        state_tensor = state.flatten().to(DEVICE)\n",
    "        state_val = stateval_network(state_tensor)\n",
    "\n",
    "        # get state value of next state\n",
    "        new_state_tensor = new_state.flatten().to(DEVICE)\n",
    "        new_state_val = stateval_network(new_state_tensor)\n",
    "\n",
    "        # if terminal state, next state val is 0\n",
    "        if done:\n",
    "            new_state_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # calculate value function loss with MSE\n",
    "        val_loss = F.mse_loss(reward + DISCOUNT_FACTOR * new_state_val, state_val)\n",
    "        val_loss *= I\n",
    "\n",
    "        # calculate policy loss\n",
    "        advantage = (\n",
    "            reward / qwerty_score_total\n",
    "        )  # + DISCOUNT_FACTOR * new_state_val.item() - state_val.item()\n",
    "        policy_loss = -lp * advantage\n",
    "        print(\"lp, policy_loss\", lp, policy_loss)\n",
    "        policy_loss *= I\n",
    "\n",
    "        # Backpropagate policy\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # Backpropagate value\n",
    "        stateval_optimizer.zero_grad()\n",
    "        val_loss.backward()\n",
    "        stateval_optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            print(\"fail\\n\")\n",
    "            break\n",
    "\n",
    "        # move into new state, discount I\n",
    "        state = new_state\n",
    "        prev_states.append((state.low_layout, state.high_layout))\n",
    "        I *= DISCOUNT_FACTOR\n",
    "\n",
    "    # append episode score\n",
    "    scores.append(score)\n",
    "    recent_scores.append(score)\n",
    "\n",
    "    # early stopping if we meet solved score goal\n",
    "    if best_score <= SOLVED_SCORE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18239.99960937735]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8286.2001953125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High layout:\n",
      "~       !       @       #       $       %       ^       &       *       (       )       _       <space> <back>  \n",
      "<tab>   Q       W       E       S       T       Y       d       I       O       P       {       }       |       \n",
      "<caps>  A       R       D       F       G       H       J       K       L       :       \"       k       <enter> \n",
      "<shift> <shift> Z       X       C       V       B       N       M       <       >       ?       <ctrl>  <shift> \n",
      "<shift> <alt>   <space> <space> <space> <space> <space> <space> <space> <alt>   <ctrl>  \n",
      "\n",
      "\n",
      "Low layout:\n",
      "`       1       2       3       4       5       6       7       8       9       0       -       =       <back>  \n",
      "<tab>   q       w       e       r       t       y       u       i       o       p       [       ]       \\       \n",
      "<caps>  a       s       U       f       g       h       j       <enter> l       ;       '       <enter> <enter> \n",
      "<shift> <shift> z       x       c       v       b       n       m       ,       .       /       <shift> <shift> \n",
      "<ctrl>  <alt>   <space> <space> <space> <space> +       <space> <space> <alt>   <ctrl>  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(best_keyboard.get_string_layouts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10272.600000000048"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwerty_score_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
